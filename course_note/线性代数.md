# linear algebra

[TOC]


## Equivalent Statements

for a n by n square matrix $A$. the following equations 

- $A$ is invertible($\iff$ A is left invertible $\iff$ A is right invertible)
- $A\vec{x}=\vec{0}$ has only the trivial solution
- the reduced echelon form of A is $I_n$
- A is expressible as a product of elementary matrices
- $\forall \vec{b},\ A\vec{x}=\vec{b}$ is consistent
- $\forall \vec{b},\ A\vec{x}=\vec{b}$ has unique solution
- $\det(A) \neq 0$
- row(column) vectors of $A$ are linearly independent
- ?



the important ones( $\text{invertible mapping} \equiv \text{bijection}$, $\text{column}\equiv \text{row}$): 

- $A^{-1}$ exists
- $\det(A)\neq 0$
- $\forall \vec{b},\ A\vec{x}=\vec{b}$ has unique solution(special case,$\vec{b}=\vec{0}$,only trivial solution)
- linearly independent vectors



## elimination & determinant & inversion

$$
\begin{aligned}
&\begin{array}{c:c}
\begin{pmatrix}
1&2&3&4&\dots &n\\
&1&2&3&\dots &n-1\\
&&1&2&\dots &n-2\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\\
&&&&&1
\end{pmatrix}
&
\begin{pmatrix}
1\\
&1\\
&&1\\
&&&\dots\\
&&&&1\\
\end{pmatrix}
\end{array}\\
\implies&\begin{array}{c:c}
\begin{pmatrix}
1&1&1&1&\dots &1\\
&1&1&1&\dots &1\\
&&1&1&\dots &1\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\\
&&&&&1
\end{pmatrix}
&
\begin{pmatrix}
1&-1\\
&1&-1\\
&&1&-1\\
&&&\dots\\
&&&&1\\
\end{pmatrix}
\end{array}\\
\newline
\implies &\begin{array}{c:c}
\begin{pmatrix}
1&0&0&0&\dots &0\\
&1&0&0&\dots &0\\
&&1&0&\dots &0\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\\
&&&&&1
\end{pmatrix}
&
\begin{pmatrix}
1&-2&1\\
&1&-2&1\\
&&1&-2&1\\
&&&\dots\\
&&&&1\\
\end{pmatrix}
\end{array}\\
\end{aligned}
$$

---------------------------------

$$
\begin{aligned}
&\det\begin{pmatrix}
a&b&b&b&b\\
b&a&b&b&b\\
b&b&a&b&b\\
b&b&b&a&b\\
b&b&b&b&a
\end{pmatrix}\\
=&\det\begin{pmatrix}
a+4b&a+4b&a+4b&a+4b&a+4b\\
b&a&b&b&b\\
b&b&a&b&b\\
b&b&b&a&b\\
b&b&b&b&a
\end{pmatrix}\\
=&\det\begin{pmatrix}
a+4b&0&0&0&0\\
b&a-b&0&0&0\\
b&0&a-b&0&0\\
b&0&0&a-b&0\\
b&0&0&0&a-b
\end{pmatrix}\\
&\text{expand along the first row}\\
&(a+4b) (-1)^{1+1}\det\begin{pmatrix}a-b\\&a-b\\
&&a-b\\
&&&a-b\end{pmatrix}=(a+4b)(a-b)^4
\end{aligned}
$$

---------------------------------

$$
\begin{aligned}
&\det\begin{pmatrix}
1&1&1&1&1\\
1&1\\
1&&1\\
1&&&1\\
1&&&&1
\end{pmatrix}\\
=&\det\begin{pmatrix}
-3&1&1&1&1\\
&1\\
&&1\\
&&&1\\
&&&&1
\end{pmatrix}\\
=&\det\begin{pmatrix}
-3&0&0&0&0\\
&1\\
&&1\\
&&&1\\
&&&&1
\end{pmatrix}\\
&\text{expand along the first row}\\
&(-3)(-1)^{1+1}\det(I_4)=-3
\end{aligned}
$$


---------------------------------

$$
\begin{aligned}
&D_n=\begin{pmatrix}
b&a\\
a&b&a\\
&a&b&a\\
&&a&b&a\\
&&\dots\\
&&&a&b&a\\
&&&&a&b
\end{pmatrix}\quad \det(D_n)\\
&\text{expand the first column}\\
&|D_n|=b\det\begin{pmatrix}
b&a\\
a&b&a\\
&a&b&a\\
&\dots\\
&&a&b&a\\
&&&a&b
\end{pmatrix}
-a\det\begin{pmatrix}
a&a\\
&b&a\\
&a&b&a\\
&\dots\\
&&a&b&a\\
&&&a&b
\end{pmatrix}\\
&|D_n|=b|D_{n-1}|-a^2\det\begin{pmatrix}
b&a\\
a&b&a\\
\dots\\
&a&b&a\\
&&a&b
\end{pmatrix}=b|D_{n-1}|-a^2|D_{n-2}|\\
\newline\\
&R_n=|D_n|\quad R_n=bR_{n-1}-a^2R_{n-2}\\
\end{aligned}
$$


---------------------------------

### Vandermonde determinant

$$
\det\begin{pmatrix}
x_1^0 & x_2^0 & x_3^0&\dots& x_n^0\\
x_1^1 & x_2^1 & x_3^1&\dots& x_n^1\\
x_1^2 & x_2^2 & x_3^2&\dots& x_n^2\\
\vdots &\vdots & \vdots& \vdots &\vdots\\
x_1^n & x_2^n & x_3^n&\dots& x_n^n\\
\end{pmatrix}=\prod_{1\leq j< i\leq n}(x_i-x_j)
$$


---------------------------------



## MISC


### 关于 $A\,adj(A)=\det(A) I$

对于任意方阵$A$,以及其代数余子式$C$,有$\sum_{k} a_{ik}C_{jk}=\begin{cases}\det(A)&(i=j)\\0&(i\neq j)\end{cases}$
$$
\begin{aligned}
&A=\begin{bmatrix}
    a_{11}&a_{12}&a_{13}&\dots &a_{1n}\\
    a_{21}&a_{22}&a_{23}&\dots &a_{2n}\\
    &&\dots \\
    a_{n1}&a_{n2}&a_{n3}&\dots &a_{nn}\\
\end{bmatrix}\\
&\text{its minors,cofactors } M_{ij},C_{ij}=(-1)^{i+j}M_{ij}\\
&\det(A)=\sum_{i} a_{ki}C_{ki}=\sum_{i} a_{ik}C_{ik}\\
\newline
&A^{\prime}=\text{(change the i-th row to the j-th row)}\\
&\text{expand along the j-th row}\\
&\det(A')=\sum_{k} (A^{\prime})_{jk}C_{jk}=\sum_{k} a_{ik}C_{jk}=0\\
\end{aligned}
$$

  

### 三角不等式.(实数或者向量)

$$
\begin{aligned}
&|| \vec a+\vec b || \leq |\vec a|+|\vec b|\quad \text{(by Cauchy-Schwarz)}\\
&\newline\\
&|| \vec{a}-\vec{b}|| +|| \vec b || \geq ||(\vec{a}-\vec{b})+\vec{b}||=||\vec a||\\
&\iff || \vec a - \vec b ||  \geq ||\vec a|| - ||\vec b||\\
&\newline\\
&||\vec u-\vec x||+||\vec x-\vec v|| \geq ||(\vec u-\vec x)+(\vec x-\vec v)||=||\vec u-\vec v||
\end{aligned}
$$

> 补充,关于Cauchy-Schwarz inequality的proof  
> $$
> \begin{aligned}
> &\forall \vec a,\vec b \quad f(x)=||\, x\vec a-\vec b\,||\\
> &f(x)\geq 0,f^2(x)=|\vec a|^2\, x^2-2\vec a\cdot \vec b\, x+|\vec b|^2\geq 0\\
> \implies &|\vec a|^2 x^2+(-2\vec a\cdot \vec b) x+|\vec b|^2=0\quad \text{has no or only one solution,}\\
> &(-2\vec a\cdot \vec b)^2-4|\vec a|^2|\vec b|^2\leq 0\\
> \implies &(\vec a\cdot\vec b)\leq |\vec a|^2|\vec b|^2\implies \vec a\cdot \vec b\leq |\vec a||\vec b|
> \end{aligned}
> $$
> 
> 当且仅当$\exists x,\vec b=x\vec a$时可以取等.




### 最小二乘法(method of least square)
$$
\min_{\vec x\in \mathbb R^n} ||A\vec x-\vec b||
$$
考虑$A\vec x$的意义,是$A$的列向量的线性组合.$\{A\vec x\mid x\in \mathbb R^n\}=span(\{\vec a_1,\vec a_2\dots \vec a_n\})$是一个不超过$n$维的超平面.  
所以这个$||A\vec x-\vec b||$就是超平面上向量到$\vec b$的距离,取$\min$就是$\vec b$到平面的距离,我们要做投影.  
找到超平面的法向量$\vec n$,则距离为$|(\vec n,\vec b)|$.  

~~至于法向量怎么求....目前不会~~  
求法向量很简单,找一个$\{Ax\mid x\in \mathbb R^n\}$的basis $B=(b_1,b_2\dots b_m)$有$n\perp b_i$.  

考虑矩阵$B=\begin{bmatrix}b_1&b_2&\dots &b_m\end{bmatrix}$


$$
\begin{aligned}
&\vec 0=\begin{bmatrix}
(n,b_1)\\
(n,b_2)\\
\vdots\\
(n,b_m)
\end{bmatrix}
=\begin{bmatrix}
{b_1}^{\mathrm T}\ n\\
{b_2}^{\mathrm T}\ n\\
\vdots\\
{b_m}^{\mathrm T}\ n\\
\end{bmatrix}
=\begin{bmatrix}
{b_1}^{\mathrm T}\\
{b_2}^{\mathrm T}\\
\vdots\\
{b_m}^{\mathrm T}\\
\end{bmatrix}n=B^\mathrm T n
\end{aligned}
$$


解方程就行了.  

### dim basis rank span

基本上是一回事...  
计算技巧上,利用高斯消元,$\det$相关结论

- 如果$S$是线性无关的,并且$v\not\in span(S)$那么$S+\{v\}$也是线性无关的.
- 如果$S$是线性相关的,$\exists v\in (S-\{v\})$那么$span(S-\{v\})=span(S)$.

这告诉我们$span(\{v_1,v_2\dots v_n\})$的basis可以有两个办法得到,第一种是不断加入不能被已有向量表示的新向量;另一种是不断从中删掉被其他向量表示的向量.



### PIE and dim


> if $U,V$ are subspace of finite-dimensional vector space $W$,  
> $U\cap V=\{v\in W\mid (v\in U)\land (v\in V)\}$and $U+V=\{x+y\mid x\in U,y\in V\}$  
> prove that $\dim(U+V)=\dim(U)+\dim(V)-\dim(U\cap V)$  

考虑$U\cap V$的basis,$S=\{v_1,v_2\dots v_n\}$,有$\dim(U\cap V)=n$.  
假设$S\cup A,S\cup B$分别是$U,V$的basis且$S\cap A=S\cap B=\phi$(即向S中添加向量,使得它张成U或者V).  

此时有$A\cap B=\phi$且$A\cup B\cup S$是$U+V$的basis.于是:  

$$
\begin{aligned}
&\dim(U+V)=|A\cup B\cup S|=|A|+|B|+|S|=\\
&\dim(U)-\dim(S)+\dim (V)-\dim (S)+\dim (S)=\dim(U)+\dim(V)-\dim(U\cap V)
\end{aligned}
$$

下面分别给出$A\cap B=\phi$和$A\cup B\cup S$是$V$的basis的证明.

- 如果$A\cap B=\{x_1,x_2\dots x_m\}\neq \phi$,那么$A\subset U,B\subset V$  
  故$(A\cap B)\subset (U\cap V)$即$x_i\in U\cap V$,因为$S$是$U\cap V$的basis,所以$x_i\in span(S)$于是$A\cup S$并非线性无关,  
  这与$A\cup S$是$U$的basis构成矛盾,故假设错误.
- 显然$span(A\cup S\cup B)=U+V$首先$(A\cup S\cup B)\subset U+V$于是$span(A\cup B\cup S)\subset U+V$,只需要证明$\forall y\in U+V,y\in span(A\cup S\cup B)$即可,  
  按照定义进行拆解$\exist u\in U,v\in V\ \text{ s.t. }\ u+v=y$有$u\in span(A\cup S)\to u\in span(A\cup S\cup B)$同理$v\in span(A\cup S\cup B)$.任意set of vectors的span都是个subspace,于是加法是封闭的,那么$u+v=y\in span(A\cup S\cup B)$.



这个性质还可以扩展$\dim(\sum W_i)\quad W_i\text{ is a subspace of }V$.



### change of basis

考虑$v=\sum_{i=1}^n x_i\vec{e_i}$其中$e$是standard basis.我们定义$v$的coordinate是$(x_1,x_2\dots x_n)$一个$n\times 1$的row vector.    
这里记$(v)_{E}=(x_1,x_2\dots x_n)$即以$E$为basis意义下的coordinate.

假设$S=\{s_1,s_2\dots s_n\}$也是空间的basis,我们要求出$y=(v)_S=(y_1,y_2\dots y_n)$其中$v=\sum_{i=1}^n y_i\vec{s_i}$


$$
\begin{aligned}
&v=\begin{bmatrix}
x_1\\ x_2\\ \vdots\\ x_n \end{bmatrix}
=\begin{bmatrix}s_1&s_2&\dots &s_n\end{bmatrix}
\begin{bmatrix}y_1\\ y_2\\ \vdots\\ y_n\end{bmatrix}
\newline\\
&\text{let }S=\begin{bmatrix}s_1&s_2&\dots &s_n\end{bmatrix}\\
&v=\vec{x}=S\vec{y}\implies y=S^{-1}x
\end{aligned}
$$


下面考虑change of basis,一个向量$x=(x_1,x_2\dots x_n)$分别在$A,B$表示为$a=(x)_A,b=(x)_B$.  
$$
\begin{aligned}
&x=A\ (x)\!_A=B\ (x)\!_B\\
&B^{-1}A\ (x)\!_A=B^{-1}B\ (x)\!_B\\
&(x)\!_B=B^{-1}A\ (x)\!_A
\end{aligned}
$$
称$B^{-1}A$为$P_{A\to B}$是A到B的transition matrix.  
简单一点的想法是向量$x$在$A$下表示为$y$,那么换到standard basis下面表示就是$Ay$. 一个standard matrix下的向量要转换到$B$为basis,那么就要左边乘一个$B^{-1}$于是就是$B^{-1}A\ y$了


### intersection of column space

> $\mathrm{span}(v_1,v_2\dots v_n)=\mathrm{col}(\begin{bmatrix}v_1,v_2\dots v_n\end{bmatrix})$ 

$$
\begin{aligned}
&A=\begin{bmatrix}a_1&a_2\dots &a_n\end{bmatrix}\quad B=\begin{bmatrix}b_1&b_2\dots &b_n\end{bmatrix}\\
&\mathrm{col}(A)\cap \mathrm{col}(B)\\
\newline\\
&x\in \mathrm{col}(A)\cap \mathrm{col}(B)\iff \exist y_1,y_2\text{ s.t. }x=Ay_1=By_2\\
&Ay_1-By_2=0\quad \begin{bmatrix}A\mid -B\end{bmatrix}\begin{bmatrix}y_1\\y_2\end{bmatrix}=0\\
\end{aligned}
$$

我们求出$\begin{bmatrix}A\mid -B\end{bmatrix}$的null space的basis,  
是一些$\mathbb{R}^{2n}$的vector,每个vector取前一半,再左乘上$A$,就是$\mathbb{col}(A)\cap \mathbb{col}(B)$的basis了.  

remark:  
$A\mid -B$和$A\mid B$的null space是类似的.  
对于一个vector space $V$的subspace $W$显然有$\{-w\mid w\in W\}=W$.  


### 正交分解定理

> Consider a inner product space $V$ and two vectors $\vec u,\vec x\in V$ where $u\neq 0$   
> $\exist k \text{ s.t. }\vec x=k\vec u+\vec b \text{ where } (b, u)=0,\vec b\perp \vec u$

考虑把$x$投影到$u$上面去,这部分应该是$(\frac{u}{||u||},x)$



$$
\begin{aligned}
&k=\frac{(u,x)}{||\vec u||}\times \frac{1}{||u||}\\
&(x-ku,u)=(x,u)-k||u||^2=(u,x)-(u,x)=0\\
&b=x-ku
\end{aligned}
$$

更进一步地$k$是唯一的.  

$$
\begin{aligned}
&(x-ku,u)=0=(x,u)-k{||u||}^2\\
&k=\frac{(x,u)}{||u||^2}
\end{aligned}
$$




#### 关于$W+W^\perp$

> Given that $W \text{ is a subspace of } V\quad \dim(V)=n$  
> Prove that $W+W^\perp=V$

首先是$W$为零空间$\{\vec 0\}$这是trivial的.  

Let $B=(u_1,u_2\dots u_m)$ be a basis of $W$.  


$$
\begin{aligned}
&A=\begin{bmatrix} u_1&u_2&u_3&\dots &u_m \end{bmatrix}\\
&0=Ax=A\begin{bmatrix} x_1\\x_2\\x_3\\\dots \\x_n \end{bmatrix}=
\begin{bmatrix}
u_1\cdot x\\
u_2\cdot x\\
u_3\cdot x\\
\vdots\\
u_m\cdot x\\
\end{bmatrix}\\
\end{aligned}
$$

So the null space of $A$ is the orthogonal completement of $W$.  

$rank(A)+nullity(A)=n$ where $rank(A)=rank(A^T)=m$ since the $m$ row vectors can form a basis, they are linearly independent.  

nullity of nullspace = dim of nullspace = dim of orthogonal completement.  
$\dim(W)+\dim(W^\perp)=n$, and $\dim(W\cap W^\perp)=\dim(\{\vec 0\})=0$ so.  

$$
\dim(W+W^\perp)=\dim(W)+\dim(W^\perp)-\dim(W\cap W^\perp)=n
$$

and $W+W^\perp\subseteq V$, so $V=W+W^\perp$




#### rank + nullity

rank是极大无关行向量集合的大小,初等行变换不改变它,消元到RREF,leading ones所在的row vector就是一组row vecs的basis.  

另一个定理告诉我们某个列向量子集线性无关,那么任意进行行变换,这些列向量仍然线性无关(但是行变换是可以改变列空间的).于是包含leading varible的column对应的column vector线性无关,再原矩阵中找到这些行也是线性无关的. 两者都是count of leading varible于是row rank=column rank.  


nullity的话,是RREF中free varibles的数目,于是rank+nullity=count of column.  







