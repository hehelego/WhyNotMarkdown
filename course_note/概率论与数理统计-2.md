# 概率论与数理统计 note. part 2

## ch7 Joint Distribution

## ch8 Transformations

### PDF transformation in variable substitution (with good smooth,bijective,differentiable properties)

For two continuous random variables $X,Y$ such that $X=g(Y)$.  
Let $F_X(x)=P(X<x),F_Y(y)=P(Y<y)$ be the CDFs of $X,Y$ respectively
and $f_X(x)=\lim_{\delta\to 0}\frac{P(|X-x|<\delta)}{2\delta}=\lim_{\delta\to 0}\frac{F_X(x+\delta)-F_X(x-\delta)}{2\delta}=F_X'(x),f_Y(y)=F_Y'(y)$ be the PDFs.  

Consider the probability that $X$ falls in $I_X=(x,x+\delta_X)$ and the corresponding interval $I_Y=(y,y+\delta_Y)$.  
where $\delta_Y=g'(x)\delta_X$  or $\mathrm{d}y=g'(x)\mathrm{d}x$

$$
\begin{aligned}
P(X\in I_X) &= P(Y\in I_Y) \\
f_X(x)|\mathrm{d}x| &= f_Y(y)|\mathrm{d}y| \\
f_Y(y) &= f_X(x) \left|\frac{\mathrm{d}x}{\mathrm{d}y}\right|
\end{aligned}
$$

For multi-variate case, $\vec y = \vec{g}(\vec x)$, similarly, consider the probability that $X$ falls in some region $D_x$ it is identical to the probability that $Y$ falls in the transformed region $D_y$.  
We have $\mathrm{d}x_1\mathrm{d}x_2\ldots \mathrm{d}x_n=\left|\frac{\partial (y_1,y_2\ldots y_n)}{\partial (x_1,x_2\ldots x_n)}\right|\mathrm{d}y_1\mathrm{d}y_2\ldots \mathrm{d}y_n$

$$
\begin{aligned}
P(X\in D_X) &= P(Y\in D_Y)\\
f_X(\vec x)\sigma_X&= f_Y(\vec y)\sigma_Y\\
f_X(\vec x)\mathrm{d}x_1\mathrm{d}x_2\ldots\mathrm{d}x_n &= f_Y(\vec y)\mathrm{d}y_1\mathrm{d}y_2\ldots \mathrm{d}y_n\\
f_Y(\vec y) &= f_X(\vec x)\left|\frac{\partial (x_1,x_2\ldots x_n)}{\partial (y_1,y_2\ldots y_n)}\right|
\end{aligned}
$$

**note:** make use of "chain-rules" (the Jacobian of composed function is the product of Jacobian) and "inverse transformation" (Jacobian of inverse is the inverse of the Jacobian).

### uncorrelated can not imply independent

Let $X\sim \mathcal{N}(0,1)$ be a standard normal random variable,  
$Y=X^2$.  

$$
\mathrm{Cov}(X,Y)=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)
=\mathbb{E}(X^3)-\mathbb{E}(X)\mathbb{E}(X^2)
=0^3-0\mathbb{E}(X^2)
=0
$$

**Special case:** For normal r.v.s, uncorrelated is equivalent to independent.

### Covariance, Correaltion examples

> Let $X,Y\sim\mathrm{Expo}(1)$ be two i.i.d. r.v.s.  
> Find the correlation between $M=\max(X,Y)$ and $L=\min(X,Y)$.

Consider two independent arrivals with rate $\lambda=1$,  
the first arrival time has a $\mathrm{Expo}(1+1)$ distribution
and the next arrival, by memoryless property, has a $\mathrm{Expo}(1)$ distribution and its is independent of the first arrival.  

(The Poisson clock model/story)

We have $L\sim \mathrm{Expo}(2),M-L\sim\mathrm{Expo}(1)$ and $L,M-L$ is independent.

$$
\begin{aligned}
\mathrm{Cov}(L,M)
&
=\mathrm{Cov}(L,M-L+L)
=\mathrm{Cov}(L,M-L)+\mathrm{Cov}(L,L)\\
&
=0+\mathrm{Var}(L)=\frac{1}{2^2}=\frac{1}{4}\\
\mathrm{Cov}(M,M)
&
=\mathrm{Cov}(M-L+L,M-L+L)\\
&
=\mathrm{Cov}(M-L,M-L)+\mathrm{Cov}(L,L)+2\mathrm{Cov}(M-L,L)\\
&
=\frac{1}{1^2}+\frac{1}{2^2}+0
=\frac{5}{4}
\end{aligned}
$$

Thus

$$
\mathrm{Corr}(L,M)
=\frac{\mathrm{Cov}(L,M)}
{\sqrt{\mathrm{Cov}(L,L)\mathrm{Cov}(M,M)}}
=\frac{1/4}{\sqrt{\frac{1}{4}\cdot \frac{5}{4}}}
=\frac{1}{\sqrt 5}=\frac{\sqrt 5}{5}
$$

### Hyper-Geometric distribution variance

> $X\sim\mathrm{HGeom}(w,b,n)$.  
> we have $w$ white balls and $b$ black balls in a bag,  
> draw $n$ balls from that bag, count the number of white balls tobe $X$.

Let $A_i$ be the event that the $i$-th drawn ball is white, and $I_i$ be the corresponding indicator  
We have $I_i\sim \mathrm{Bern}\left(\frac{w}{w+b}\right)=\mathrm{Bern}(p)$, let $q=1-p=\frac{b}{w+b}$

$$
\begin{aligned}
\mathrm{Var}(X)
&=\mathrm{Var}\left( \sum_{i=1}^n I_i \right)
=\sum_{i=1}^n\sum_{j=1}^n \mathrm{Cov}(I_i,I_j)\\
&=\sum_{i=1}^n \mathrm{Var}(I_i)+2\sum_{i<j}\mathrm{Cov}(I_i,I_j)\\
\text{(Symmetry)}
&=n\mathrm{Var}(I_1)+2\binom{n}{2}\mathrm{Cov}(I_1,I_2)\\
\mathrm{Var}(I_i)
&=pq=\frac{w}{w+b}\frac{b}{w+b}\\
\mathrm{Cov}(I_1,I_2)
&=\mathbb{E}(I_1 I_2)-\mathbb{E}(I_1)\mathbb{E}(I_2)\\
&=\mathbb{P}(A_1\cap A_2)-\mathbb{P}(A_1)\mathbb{P}(A_2)\\
&=\frac{w}{w+b}\frac{w-1}{w+b-1}-p^2\\
\end{aligned}
$$

### multi-variate normal: MVN

A random vector $\vec X=(X_1,X_2\ldots X_n)$ is said to be a MVN
iff $\mathrm{span}(X_1,X_2\ldots X_n)$, all the linear combination of the components, all have normal distribution (a constant value $c$ is viewed as $\mathcal{N}(c,0)$)

#### examples

- $X\sim \mathcal{N}(0,1), Y=SX, P(S=1)=P(S=-1)=\frac{1}{2}$.  
  The marginal distribution of $X$ and $Y$ are both $\mathcal{N}(0,1)$,  
  but $(X,Y)$ is not a MVN since $P(X+Y=0)=P(S=-1)=\frac{1}{2}$.
- $X,Y\sim \mathcal{N}(0,1), Z=X+Y$ then $(X,Y,Z)$ is a MVN, although $Z$ is not independent from $(X,Y)$

#### BVN generating

Generate a BVN $(U,V)$ from two independent $\mathcal{N}(0,1)$ r.v.s $X,Y$, where $U,V\sim \mathcal{N}(0,1)$ and $\mathrm{Cov}(U,V)=\rho$  

Suppose that $U=aX+bY,V=cX+dY$

$$
\begin{cases}
\mathbb{E}(U)=\mathrm{E}(V)=a+b=c+d=1\\
\mathrm{Var}(U)=\mathrm{Var}(V)=a^2+b^2=c^2+d^2=1\\
\mathrm{Cov}(U,V)=\mathrm{Cov}(aX+bY,cX+dY)
=ac\mathrm{Var}(X)+bd\mathrm{Var}(Y)+0+0
=ac+bd
=\rho
\end{cases}
$$

Then

$$\begin{cases}
a=1\\
b=0\\
c=\rho\\
d=\sqrt{1-\rho^2}
\end{cases}
$$
is a solution.

#### BVN MGF

Let $(X,Y)$ be a BVN, where the marginal distribution is
$X\sim\mathcal{N}(\mu_x,\sigma_x^2)$,$Y\sim\mathcal{N}(\mu_y,\sigma_y^2)$
and $\mathrm{Corr}(X,Y)=\rho$.  
Then $Z=t_x X + t_y Y$ is a normal random variable $Z\sim\mathcal{N}(\mu,\sigma^2)$.

$$
\begin{aligned}
\mu
&=\mathbb{E}(Z)=t_x \mathbb{E}(x)+t_y \mathbb{E}(y) = t_x\mu_x+t_y\mu_y\\
\sigma^2
&=\mathrm{Var}(Z)=\mathrm{Cov}(t_x X+ t_y Y,t_x X+ t_y Y)
=t_x^2 \sigma_x^2 +t_y^2 \sigma_y^2+2t_xt_y\mathrm{Cov}(X,Y)\\
M_{X,Y}(t_x,t_y)
&=\mathbb{E}(e^{t_x X+ t_y Y})=\mathbb{E}(e^Z)
=\exp\left(
\mu + \frac{1}{2}\sigma^2
\right)\\
&=\exp\left(
\mu_x t_x + \mu_y t_y
+\frac{1}{2}
\left(
\sigma_x^2 t_x^2 + \sigma_y^2 t_y^2
+2\sigma_x t_x \sigma_y t_y \rho
\right)
\right)\\
\end{aligned}
$$

The marginal MGF of $X,Y$ are

$$
\begin{aligned}
M_X(t_x)
&=\exp\left( \mu_x t_x  + \frac{1}{2}\sigma_x^2 t_x^2 \right)\\
M_Y(t_y)
&=\exp\left( \mu_y t_y  + \frac{1}{2}\sigma_y^2 t_y^2 \right)\\
M_X(t_x)M_Y(t_y)
&=\exp\left( \mu_x t_x+\mu_y t_y  + \frac{1}{2}(\sigma_x^2 t_x^2 + \sigma_y^2 t_y^2) \right)\\
\end{aligned}
$$

The joint/marginal MGF determines the joint/marginal distribution.  

$\mathrm{Corr}(X,Y)=\rho=\frac{\mathrm{Cov}(X,Y)}{\sigma_x \sigma_y}=0$  
**iff**  
the product of marginal MGFs is equal to the joint MGF.  
**iff**  
the product of marginal PDFs is equal to the joint PDF.  

This is a significant result: In MVN, uncorrelated is equivalent to independent.  
(Generally speaking, uncorrelated is weaker than independent).

### normal samples: sample mean and sample variance are independent

$X_1,X_2\ldots X_n\sim \mathcal{N}(\mu,\sigma^2)$ are $n$ i.i.d. r.v.s.  
Therefore $\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i$ is a $\mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right)$ random variable.  

Consider $\vec{X}=(\bar X,X_1-\bar X,X_2-\bar X\ldots X_n-\bar X)$ is a linear combination of $(X_1,X_2\ldots X_n)$, which is a MVN.
Thus $\vec{X}$ is a MVN.  
Consider the covariance between $\bar X$ and $X_i-\bar X$.  

$$
\begin{aligned}
\mathrm{Cov}(\bar X,X_k-\bar X)
&=\mathrm{Cov}(\bar X,X_k)-\mathrm{Cov}(\bar X,\bar X)\\
&=\frac{1}{n}\mathrm{Cov}\left(\sum_{i=1}^n X_i, X_k\right) - \frac{\sigma^2}{n}\\
&=-\frac{\sigma^2}{n}+\frac{1}{n}\sum_{i=1}^n \mathrm{Cov}(X_i,X_k)\\
&=-\frac{\sigma^2}{n}+\frac{1}{n}\sum_{i=1}^n [i=k]\sigma^2\\
&=-\frac{\sigma^2}{n}+\frac{\sigma^2}{n}=0
\end{aligned}
$$

In MVN, uncorrelated components are independent.  
So $\bar X$ is independent of $X_i-\bar{X}$ for all $1\leq i\leq n$.  
(sample mean, distance from mean are independent)

Consider $S_n=\frac{1}{n-1}\sum_{i=1}^n {(X_i-\bar X)}^2$, is a function of $(X_1-\bar X,X_2-\bar X,\ldots X_n-\bar X)$  
so it is independent of $\bar{X}$.  
Sample mean and sample variance are independent.

### generating Normals, the Box-Muller method

Let $U\sim \mathrm{Unif}(0,2\pi), T\sim \mathrm{Expo}(1)$ be independent r.v.s  

$$
\begin{cases}
X=\sqrt{2 T}\cos U\\
Y=\sqrt{2 T}\sin U\\
\end{cases}
$$

Then $X,Y$ are i.i.d. $\mathcal{N}(0,1)$ r.v.s

$$
\begin{aligned}
\begin{vmatrix} \frac{\partial (x,y)}{\partial (u,t)} \end{vmatrix}
&=\begin{vmatrix}
-\sqrt{2t}\sin u  &  \frac{\cos u}{\sqrt{2t}} \\
 \sqrt{2t}\cos u  &  \frac{\sin u}{\sqrt{2t}} \\
\end{vmatrix}=-1\\
f_{X,Y}(x,y)
&=f_{U,T}(u,t) \begin{Vmatrix} \frac{\partial (u,t)}{\partial (x,y)} \end{Vmatrix}
=f_{U}(u)f_{T}(t) \begin{Vmatrix} \frac{\partial (u,t)}{\partial (x,y)} \end{Vmatrix}\\
&=\frac{1}{2\pi} \exp{\left(-\frac{x^2+y^2}{2}\right)}\frac{1}{|-1|}\\
&=\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2}\cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}y^2}\\
\implies f_{X,Y}(x,y)&=f_X(x)f_Y(y)
\quad
\begin{cases}
f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2}\\
f_Y(y) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}y^2}
\end{cases}
\end{aligned}
$$

## ch9 Conditional Expectation

### example: conditional expectation inequality (average life span)

$$
\mathbb{E}(X|X\geq t)\geq \mathbb{E}(X)
$$

$$
\begin{aligned}
\mathbb{E}(X)
&=\mathbb{E}(X|X\geq t)P(X\geq t)+\mathbb{E}(X|X<t)P(X<t)\\
&=\mathbb{E}(X|X\geq t) - \mathbb{E}(X|X\geq t)P(X<t)+\mathbb{E}(X|X<t)P(X<t)\\
&=\mathbb{E}(X|X\geq t) + \left( \mathbb{E}(X|X<t)-\mathbb{E}(X|X\geq t) \right) P(X<t)\\
&\leq \mathbb{E}(X|X\geq t) + \left( t-t \right) P(X<t)\\
&=\mathbb{E}(X|X\geq t)
\end{aligned}
$$

### Adam and Eve (iterated conditional expectation, conditional variance)

$$
\begin{aligned}
\mathbb{E}\left(
\mathbb{E}\left(X|Y\right)
\right)
&=\mathbb{E}(X)\\
\mathbb{E}\left(
\mathbb{E}(X|Y,Z)
|Z
\right)
&=\mathbb{E}(X|Z)\\
\operatorname{Var}(X)
&=\mathbb{E}\left( \operatorname{Var}(X|Y)) \right)
+\operatorname{Var}\left( \mathbb{E}(X|Y)) \right)
\end{aligned}
$$

#### proof: conditional expectation

For the iterated conditional expectation.  
This is a direct implication of the LOTE (law of total expectation)  
Can be intepreted as **The total expectation is the weighted average of expectation within a group**.

$$
\begin{aligned}
\mathbb{E}\left(
\mathbb{E}(X|Y)
\right)
&=\sum_{y}\mathbb{E}(X|Y=y)P(Y=y)=\mathbb{E}(X)\\
\end{aligned}
$$

Let $\hat{\mathbb{E}}(\cdot)=\mathbb{E}(\cdot | Z)$, we have
$\mathbb{\hat E}\left(\mathbb{\hat E}\left(X|Y\right)\right)$.

#### proof: conditional variance

For the conditional variance law.  
This can be proved using the iterated conditional expectation property.  

Can be interpreted as **the total variance is the sum of variance between groups and the expectation of variance within a group**

$$
\begin{aligned}
\mathbb{E}\left( \operatorname{Var}(Y|X) \right)
&=\mathbb{E}\left(
  \mathbb{E}(Y^2|X)-{\left( \mathbb{E}(Y|X) \right)}^2
\right)
=\mathbb{E}(Y^2)
-\mathbb{E}\left(
  {\left( \mathbb{E}(Y|X) \right)}^2
\right)\\
\operatorname{Var}\left(\mathbb{E}\left(X|Y\right)\right)
&=\mathbb{E}\left(
{\left( \mathbb{E}(X|Y) \right)}^2
\right)
-{\left( \mathbb{E}
\left( \mathbb{E}\left(X|Y\right) \right)
\right)}^2
=\mathbb{E}\left(
  {\left( \mathbb{E}(Y|X) \right)}^2
\right)
-{\left( \mathbb{E}(Y) \right)}^2\\
\operatorname{Var}(Y)
&=\mathbb{E}\left( \operatorname{Var}(Y|X) \right)
+\operatorname{Var} \left( \mathbb{E}(Y|X) \right)
\end{aligned}
$$

#### examples: random sum

> We have some random variables $X_1\ldots X_N$  
> When the ranndom positive integer $N$ is given. We have $X_1,X_2\ldots X_N$ are i.i.d. with $\mathbb{E}(X_i)=\mu,\operatorname{Var}(X_i)=\sigma^2$  
> Find the mean and variance of $X=\sum_{i=1}^N X_i$

$$
\begin{aligned}
\mathbb{E}(X)
&=\mathbb{E}\left(
\mathbb{E}( X|N )
\right)
=\mathbb{E}\left(
\mathbb{E}\left( \sum_{i=1}^N X_i |N \right)
\right)
&(\text{condition on $N$})\\
&=\mathbb{E}\left(
\sum_{i=1}^N \mathbb{E}\left( X_i |N \right)
\right)
=\mathbb{E}\left( N\mu \right)=\mu \mathbb{E}\left( N\right)
&(\text{linearity})\\
\mathbb{Var}(X)
&=\mathbb{E}
\left( \operatorname{Var}(X|N) \right)
+\operatorname{Var}\left(
\mathbb{E}(X|N)
\right)
&(\text{conditional variance law})\\
&=\mathbb{E}
\left( \operatorname{Var}\left(\sum_{i=1}^N X_i|N\right) \right)
+\operatorname{Var}\left( \mu N \right)
&\text{()}\\
&=\mathbb{E}
\left( \sum_{i=1}^N \operatorname{Var}\left(X_i|N\right) \right)
+\mu^2 \operatorname{Var}(N)
&\text{(variance of sum of i.i.d. s)}\\
&=\mathbb{E}\left( N\sigma^2 \right) +\mu^2 \operatorname{Var}(N)
=\sigma^2 \mathbb{E}\left( N\right) +\mu^2 \operatorname{Var}(N)
&\text{()}\\
\end{aligned}
$$

### first-step method with examples

#### Expectation of $\mathrm{Geom}(p)$

Let $X\sim \mathrm{Geom}(p)$.
Consider a Bernoulli process, $X$ is the failures before the first success.  
Let $I$ be the indicator of the event that the first trial succeeded.  

LOTE:

$$
\begin{aligned}
\mathbb{E}(X)
&=\mathbb{E}\left( \mathbb{E}(X|I) \right) & \text{(LOTE)}\\
&=\mathbb{E}(X|I=0)\Pr(I=0)+\mathbb{E}(X|I=1)\Pr(I=1)\\
&=q\mathbb{E}(X|I=0)+p\mathbb{E}(X|I=1)\\
&=q\left( 1+\mathbb{E}(X) \right) +p\cdot 0 &\text{(memoryless, fresh start)}\\
\implies \mathbb{E}(X) &= \frac{q}{p}
\end{aligned}
$$

For $Y\sim \mathrm{FS}(p)$, we have $\mathbb{E}(Y)=p\cdot 1 + q \left( 1+\mathbb{E}(Y) \right), \mathbb{E}(Y)=\frac{1}{p}$

#### Coin tosses sequence pattern matching

> We filp a coin multiple times.
>
> - What is the expected number of tosses needed until $\mathrm{HT}$
> - What is the expected number of tosses needed until $\mathrm{HH}$
> - What is the expected number of tosses needed until $\mathrm{HTT}$
> - What is the expected number of tosses needed until one of $\mathrm{HTT},\mathrm{TTTHHT},\mathrm{HHTTHH}$
> - What is the expected number of tosses needed until all of $\mathrm{HTT},\mathrm{TTTHHT},\mathrm{HHTTHH}$
>
> keywords:  
> **DFA, KMP, Aho-Corasick**  
> **PGF probability generating function**, **random walk on graph**  
> **Markov Chain**, **Martingale**

We will tackle the first three with the help of _Conditional Expectation_ and _Memoryless property of Bernoulli Process_.  

For $T_{HT}$, this can be viewed as:  Try until we see a $\mathrm{H}$, $T_1$ trials. Continue Trying until we see a $\mathrm{T}$, $T_2$ trials.  
Memoryless property of Bernoulli process, $T_1,T_2\sim \mathrm{FS}(1/2)$ so $\mathbb{E}(T_{HT})=\mathbb{E}(T_1)+\mathbb{E}(T_2)=2+2=4$.

Let's use first-step analysis to find the expectation.  
Let $p$ be the probability of $H$ and $q$ be the probability of $T$.

- For $\mathrm{HH}$: condition on the first toss $O_1$.
  $$
  \begin{aligned}
  \mathbb{E}(T_{HH})
  &= \mathbb{E}(T_{HH}|O_1=H)\Pr(O_1=H) + \mathbb{E}(T_{HH}|O_1=T)\Pr(O_1=T)\\
  &= p\mathbb{E}(T_{HH}|O_1=H) + q\left( 1 + \mathbb{E}(T_{HH}) \right)\\
  \mathbb{E}(T_{HH}|O_1=H)
  &=\mathbb{E}(T_{HH}|O_2=H,O_1=H)\Pr(O_2=H|O_1=H)\\
  &+\mathbb{E}(T_{HH}|O_2=T,O_1=H)\Pr(O_2=T|O_1=H)\\
  &=p\mathbb{E}(T_{HH}|O_2=H,O_1=H)+q\mathbb{E}(T_{HH}|O_2=T,O_1=H)\\
  &=p\cdot 2+q\left( 2+\mathbb{E}(T_{HH}) \right)=2+q\mathbb{E}(T_{HH})\\
  \mathbb{E}(T_{HH})
  &= p\mathbb{E}(T_{HH}|O_1=H) + q\left( 1+\mathbb{E}(T_{HH}) \right)\\
  &= p\left( 2+ q\mathbb{E}(T_{HH}) \right) +  q\left( 1+\mathbb{E}(T_{HH}) \right)\\
  \implies \mathbb{E}(T_{HH}) = \frac{1+p}{p^2}
  \end{aligned}
  $$
- For $\mathbb{E}(T_{HT})$
  $$\begin{aligned}
  \mathbb{E}(T_{HT})
  &= \mathbb{E}(T_{HT}|O_1=H)\Pr(O_1=H)+\mathbb{E}(T_{HT}|O_1=T)\Pr(O_1=T)\\
  &= p\mathbb{E}(T_{HT}|O_1=H)+q\left( 1+\mathbb{E}(T_{HT}) \right)\\
  \mathbb{E}(T_{HT}|O_1=H)
  &= \mathbb{E}(T_{HT}|O_2=T,O_1=H)\Pr(O_2=T|O_1=H)\\
  &+ \mathbb{E}(T_{HT}|O_2=H,O_1=H)\Pr(O_2=H|O_1=H)\\
  &= 2\cdot q+ p\left( 2+\mathbb{E}(T_{HT}|O_1=H)-1 \right)\\
  \implies \mathbb{E}(T_{HT}|O_1=H) &= 1+\frac{1}{q}\\
  \mathbb{E}(T_{HT})
  &= p\mathbb{E}(T_{HT}|O_1=H)+q\left( 1+\mathbb{E}(T_{HT}) \right)\\
  &= p\left( 1+\frac{1}{q} \right) +q\left( 1+\mathbb{E}(T_{HT}) \right)\\
  \implies \mathbb{E}(T_{HT})=\frac{1}{p} + \frac{1}{q} = \frac{1}{pq}
  \end{aligned}$$

### projection interpretation of conditional expectation

$Y-\mathbb{E}(Y|X)$ and $h(X)$ are uncorrelated.

$$
\begin{aligned}
\operatorname{Cov}(h(X),Y-\mathbb{E}(Y|X))
&=\mathbb{E} \left( h(X)(Y-\mathbb{E}(Y|X)) \right)
-\mathbb{E}(h(X))\mathbb{E}\left( Y-\mathbb{E}(Y|X) \right)\\
&=\mathbb{E} \left( h(X)(Y-\mathbb{E}(Y|X)) \right) -\mathbb{E}(h(X))0\\
\mathbb{E}\left[ h(X)E(Y|X) \right]&=\mathbb{E} \left( h(X)(Y-\mathbb{E}(Y|X)) \right)\\
&=\mathbb{E}\left[ h(X)Y \right] -\mathbb{E}\left[h(X)\mathbb{E}(Y|X)\right]\\
&=\mathbb{E}\left[ h(X)Y \right] -\mathbb{E}\left[\mathbb{E}(h(X)Y|X)\right]\\
&=\mathbb{E}\left[ h(X)Y \right] -\mathbb{E}(h(X)Y)\\
&=0
\end{aligned}
$$

Consider this vector space $(V,F,+,\cdot)$ where

- $V=\{ X,Y\ldots \}$, is all the random variables (and all the functions of r.v.s. which are also random variables)
- $F=\mathbb{R}$

We can define a inner product on this space.

$$
\left\langle X,Y \right\rangle=\mathbb{E}(XY)
$$

We can vertify that this is a _non-negative, additive, symmetry, homogeneous_ bilinear form.

Therefore $\mathbb{E}\left[(Y-\mathbb{E}(Y|X))\, h(X)\right]=0$ can be interpreted as:  
$$
\left\langle Y-\mathbb{E}(Y|X),h(X) \right\rangle=0
\iff (Y-\mathbb{E}(Y|X)) \perp h(X)
$$

This shows that $Y-\mathbb{E}(Y|X)$ and the sub-space $U=\{h(X)\mid h:\mathbb{V}\to \mathbb{V}\}$ are orthogonal.  
The projection of $Y$ on $U=\{ h(X) \}$ is $\mathbb{E}(Y|X)$

#### MMSE

$$
\begin{aligned}
{\begin{Vmatrix} Y-g(X) \end{Vmatrix}}^2
&={\begin{Vmatrix} Y-\mathbb{E}(Y|X) + \mathbb{E}(Y|X)-g(X) \end{Vmatrix}}^2\\
&={\begin{Vmatrix} Y-\mathbb{E}(Y|X) \end{Vmatrix}}^2 + {\begin{Vmatrix} \mathbb{E}(Y|X)-g(X) \end{Vmatrix}}^2
+2\left\langle Y-\mathbb{E}(Y|X), \mathbb{E}(Y|X)-g(X)\right\rangle\\
&={\begin{Vmatrix} Y-\mathbb{E}(Y|X) \end{Vmatrix}}^2 + {\begin{Vmatrix} \mathbb{E}(Y|X)-g(X) \end{Vmatrix}}^2\\
&\leqslant {\begin{Vmatrix} Y-\mathbb{E}(Y|X) \end{Vmatrix}}^2 + 0
\qquad (\text{when $g(X)=\mathbb{E}(Y|X)$})
\end{aligned}
$$

Where $\mathbb{E}(Y|X)-g(X)$, as a function of $X$ is orthogonal to $Y-\mathbb{E}(Y|X)$.

This is the Minimum Mean Square Error (MMSE) estimator of $Y$ given $X$.  
$\operatorname{MMSE}(Y|X)=\operatorname{proj}_{\{h(X)\mid h:\mathbb{V}\to \mathbb{V}\}}(Y)=\mathbb{E}(Y|X)$.

#### uniqueness of the projection

For a function $g(X)$, $\left( \forall h,\, Y-g(X)\perp h(x) \right)\iff g(X)=\mathbb{E}(Y|X)$  
Or in language of linear algebra: $v=\operatorname{proj}_{W} u \iff v\in W\land \left(\forall x\in W,\, 0=\left\langle x,u-v \right\rangle\right)$

We will show LHS implies RHS.

Let $h(X)=g(X)-\mathbb{E}(Y|X)$ be a function of $X$,  
then $h(X)\perp \left( Y-g(X)\right)$ and $h(X)\perp \left(Y-\mathbb{E}(Y|X)\right)$

$$
\begin{aligned}
\left\langle h(X), Y-g(X) \right\rangle
&=\left\langle h(X), Y-\mathbb{E}(Y|X) \right\rangle\\
\implies 0
&=\left\langle h(X), \left( Y-g(X) \right) - \left( Y-\mathbb{E}(Y|X) \right) \right\rangle\\
&=\left\langle h(X), \mathbb{E}(Y|X)-g(X) \right\rangle\\
&=\left\langle h(X), h(X) \right\rangle\\
&={\begin{Vmatrix} h(X) \end{Vmatrix}}^2\\
\implies h(X)&=g(X)-\mathbb{E}(Y|X)=0\\
\implies g(X)&=\mathbb{E}(Y|X)\\
\end{aligned}
$$

#### LLSE

Besides the general MMSE, we also have the convenient LLSE (linear least square error estimator).  
The LLSE of $Y$ given $X$ is defined as $\operatorname{LLSE}(Y|X)=\operatorname{proj}_{\{ a+bX\mid a,b\in F \}}(Y)$.

We will introduce the method to find LLSE but skip the proof.

$$
\begin{aligned}
&\begin{cases}
\mathbb{E}(Y)
&=\mathbb{E}\left[ a+bX \right]
=a+b\mathbb{E}(X)\\
\operatorname{Cov}(X,Y)
&=\operatorname{Cov}(X,a+bX)
=b\operatorname{Var}(X)\\
\end{cases}\\
&\begin{cases}
a&=\mathbb{E}(Y) - \mathbb{E}(X)b\\
b&=\frac{\operatorname{Cov}(X,Y)}{\operatorname{Var}(X)} \\
\end{cases}
\end{aligned}
$$

#### MMSE and LLSE in BVN (Bivariate Normal Variables)

Suppose that $(X,Y)$ is a BVN i.e. any linear combination of $X,Y$ has a $\mathcal{N}(\mu,\sigma^2)$ distribution.  
Then MMSE of $Y$ given $X$ is equal to the LLSE of $Y$ given $X$. $\operatorname{MMSE}(Y|X)=\operatorname{LLSE}(Y|X)$.  

Let $\mathrm{L}(Y|X)=a+bX$, where $b=\frac{\operatorname{Cov}(X,Y)}{\operatorname{Var}(X)}, a=\mathbb{E}(Y)-\mathbb{E}(X)\frac{\operatorname{Cov}(X,Y)}{\operatorname{Var}(X)}$.  

$$
\begin{aligned}
\mathrm{L}(Y|X)
&=\mathbb{E}(Y)-\mathbb{E}(X)\frac{\operatorname{Cov}(X,Y)}{\operatorname{Var}(X)}
+X\frac{\operatorname{Cov}(X,Y)}{\operatorname{Var}(X)}\\
&=\mathbb{E}(Y)+(X-\mathbb{E}(X))\frac{\operatorname{Cov}(X,Y)}{\operatorname{Var}(X)}
\end{aligned}
$$

1. $Y-\mathrm{L}(Y|X)\perp X$ since $\mathrm{L}(Y|X)$ is the projection of $Y$ on $\{a+bX\mid a,b\in F\}$
2. $Z=Y-\mathrm{L}(Y|X)=Y-(a+bX)$ is a normal random variable since $(X,Y)$ is a BVN.  
   $(Z,X)$ is also a BVN as every linear combination of $Z,X$ is a linear combination of $X,Y$.
3. $Z,X$ are uncorrelated.
   $$\begin{aligned}
   \operatorname{Cov}(Z,X)
   &=\mathbb{E}(XZ)-\mathbb{E}(X)\mathbb{E}(Z)\\
   &=\left\langle X,Z\right\rangle -\mathbb{E}(X)\cdot 0\\
   &=0-0=0
   \end{aligned}$$
   In BVN, uncorrelated implies independent.
4. $Z=Y-\mathrm{L}(Y|X)$ is independent of $X$, so it is independent of any function of $X$, $h(X)$.  
   $Z,h(X)$ independent implies $\operatorname{Cov}(Z,h(X))=0$.
5. Consider the inner product of $h(X)$ and $Y-\mathrm{L}(Y|X)$, we want to show that they are orthogonal.
   $$\begin{aligned}
   \left\langle Y-\mathrm{L}(Y|X), h(X) \right\rangle
   &=\operatorname{Cov}(Y-\mathrm{L}(Y|X),h(X))+\mathbb{E}h(X)\mathbb{E}(Y-\mathrm{L}(Y|X))\\
   &=0+\mathbb{E}h(X)\cdot 0=0
   \end{aligned}$$
6. Therefore, $Y-\mathrm{L}(Y|X)$ is orthogonal to $\{h(X)\mid h:V\to V\}$.  
   $\mathrm{L}(Y|X)$ is the projection of $Y$ on $\{h(X)\mid h:V\to V\}$.  
   so $\mathrm{L}(Y|X)=\mathbb{E}(Y|X)$.

- The projection on function space gives MMSE (best approximation).  
- The projection on linear function spaec gives LLSE (best linear approximation).  

## ch10 Inequalities and Limit Theorems

## ch11 Markov Chain

## ch12 MCMC

## ch13 Poisson Process

## MISC

### Poisson approximation

If $X_n\sim \mathrm{Bin}(n,\frac{\lambda}{n})$ then as $n\to \infty$, we have $X_{\infty}\sim \mathrm{Pois}(\lambda)$.  

For $n$ (weakly) independent rare events $A_1,A_2\ldots A_n$ where $\mathbb{E}(I(A_i))=P(A_i)=p_i$ is small,
and $\lambda=\sum_{i} p_i$.
Let $X=\sum_{i}I(A_i)$ approximately, has a Poisson distribution $\mathrm{Pois}(\lambda)$.

### Poisson process and Exponential distribution

- number of arrivals in an interval of length $t$ is a $\mathrm{Pois}(\lambda t)=e^{-\lambda t}\frac{{(\lambda t)}^n}{n!}$.
- time between two arrivals has an Exponential distribution.
- disjoint intervals are independent process.
- sum of poisson process is a poisson process.
- $X,Y$ are independent Poisson r.v.s, then $X+Y$ has a Poisson distribution and $X|X+Y=n$ has a Binomial distribution.

### expected value of geometric distribution

$\Pr(X=i) = {(1-p)}^{i-1}p$ for $p=1,2,3\ldots$

#### approach 1

let $q=1-p$

$$
\begin{aligned}
E(X)
&=\sum_{i=1}^\infty i q^{i-1} (1-q)\\
&=\lim_{n\to\infty}\sum_{i=1}^n i q^{i-1} (1-q)\\
&=\lim_{n\to\infty}\sum_{i=1}^n i q^{i-1} - \sum_{i=1}^n i q^i\\
&=\lim_{n\to\infty}\sum_{i=0}^{n-1} (i+1) q^i - \sum_{i=1}^n i q^i\\
&=\lim_{n\to\infty} (0+1) q^0-n q^n +\sum_{i=1}^{n-1}q^i\\
&=1+\frac{q}{1-q} = \frac{1}{1-q} = \frac{1}{p}
\end{aligned}
$$

#### approach 2

$$
\begin{aligned}
E(X)
&=\sum_{i=1}^\infty i\Pr(X=i)\\
&=\lim_{n\to \infty}\sum_{i=1}^n i\Pr(X=i)\\
&=\lim_{n\to \infty}\sum_{i=1}^n \Pr(X=i)\sum_{j=1}^i 1\\
&=\lim_{n\to \infty}\sum_{j=1}^n \sum_{i=j}^n \Pr(X=i)\\
&=\lim_{n\to \infty}\sum_{j=1}^n \Pr(j\leq X\leq n)\\
(?)&=\sum_{j=1}^\infty \Pr(X\geq j)\\
\end{aligned}
$$

$$
\begin{aligned}
\Pr(X\geq i)
&={(1-p)}^{i-1}\\
E(X)
&=\sum_{i=1}^\infty {(1-p)}^{i-1}=\frac{1}{1-(1-p)}=\frac{1}{p}
\end{aligned}
$$

#### extension

Similarly, for positive continuous random variable $X$ and its PDF $f(x)$:

$$
E(X)
=\int_0^{+\infty} xf(x)\mathrm{d}x
=\int_0^{+\infty} \int_0^x 1 f(x)\mathrm{d}y\mathrm{d}x
=\int_0^{+\infty} \Pr(X\geq x)\mathrm{d}x
$$

### conditional independence in bayesian network

see

- [zhihu王乐: 概率图模型之贝叶斯网络](https://zhuanlan.zhihu.com/p/30139208)
- wikipedia
- conditional probability chain rule:  
$$
\begin{aligned}
P(A_1A_2A_3\ldots A_n)
&=P(A_1)P(A_2\ldots A_n |A_1)\\[1em]
&=P(A_1)\ P(A_3\ldots A_n |A_1,A_2)P(A_2 |A_1)\\
&=P(A_1)P(A_2)P(A_3\ldots A_n |A_1,A_2)P(A_2 |A_1)\\[1em]
&=P(A_1)P(A_2|A_1)\ P(A_4\ldots A_n|A_1,A_2,A_3)P(A_3|A_1A_2)\\
&=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)P(A_4\ldots A_n|A_1,A_2,A_3)\\[1em]
&=\ldots\\
&=\prod_{i=1}^n P(A_i|A_1\ldots A_{i-1})
\end{aligned}
$$

#### the structure and data in the network

- DAG, random variable on the vertices, conditional probability on the edges.
- suppose that $A_1,A_2\ldots A_n$ are the ancestors of $B$ in $G=(E,V)$  
  i.e. $\{A_1,A_2\ldots A_n\}=\{v\in V\mid (v,B)\in E\}$  
  then we have a conditional distribution $P(C| A_1\ldots A_n)$  
  $P(C=c_i| A_1=a_1,A_2=a_2\ldots A_n=a_n)$

#### head to head

```plaintext
A -> X
B -> X
C -> X
```

the network(graph) gives $P(X=x| A=a,B=b,C=c)$

#### tail to tail

```plaintext
X -> A
X -> B
X -> C
```

the network(graph) gives $P(A=a| X=x), P(B=b| X=x), P(C=c| X=x)$

#### head to tail

```plaintext
A -> B -> C
```

the network(graph) gives $P(B=b| A=a), P(C=c| B=b)$

### PGF example: first occurance of a given pattern in coin filp sequence

> Given a sequence $(a_1,a_2\ldots a_n\ldots )$
> where $a_i$ are i.i.d. Bernoulli random variable with prameter $p$, let $q=1-p$.  
>
> Let $X$ be the first time that $111$ is witnessed i.e. $X=k$ iff $(a_{k-2},a_{k-1},a_{k})=(1,1,1)$ and $\forall i<k, (a_{i-2},a_{i-1},a_i)\neq (1,1,1)$.  
>
> Find the moments of $X$.

Let $r_n=P(X=n)$ for all natural number $n$. We have the initial condition $p_0=p_1=p_2=0,p_3=p^3$.  

For every $n>3$.  
Condition on the first step (condition on $a_1$), LOTP:

$$
\begin{aligned}
r_n = P(X=n)
&=P(X=n|a_1=0)P(a_1=0)+P(X=n|a_1=1)P(a_1=1)\\
&=q P(X=n|a_1=0) + pP(X=n|a_1=1)
\end{aligned}
$$

- For $P(X=n|a_1=0)$, since the Bernoulli process is memory less, this is equivalent to $P(X=n-1)=r_{n-1}$
- For $P(X=n|a_1=1)$, we have to consider $a_2$.  
  1. If $a_2=0$, this is another fresh start $P(X=n|a_1=1,a_2=0)=P(X=n-2)$
  2. If $a_2=1$, then $a_3=0$, otherwise we would have $X=3$, $P(X=n|a_1=1,a_2=1)=P(X=n-3)P(a_3=0)$

Thus (LOTP with extra condition):

$$
\begin{aligned}
&q P(X=n|a_1=0) + p P(X=n|a_1=1)\\
=&q r_{n-1} + p
  \left[
    P(X=n|a_1=1,a_2=0)P(a_2=0|a_1=1)
   +P(X=n|a_1=1,a_2=1)P(a_2=1|a_1=1)
  \right]\\
=&q r_{n-1} + p
  \left[
    P(X=n|a_1=1,a_2=0)q
   +P(X=n|a_1=1,a_2=1)p
  \right]\\
=&q r_{n-1} + p
  \left[
    q P(X=n-2)
   +p P(X=n|a_1=a_2=1,a_3=0) P(a_3=0|a_1=a_2=1)
  \right]\\
=&q r_{n-1} + p
  \left[
    q P(X=n-2)
   +pq P(X=n-3)
  \right]\\
=& q r_{n-1}+pq r_{n-2} + p^2 q r_{n-3}
\end{aligned}
$$

Let $g(z)=\mathbb{E}(z^X)=\sum_{n=0}^\infty r_n z^n$ be the PGF of $X$,

$$
\begin{aligned}
g(z)&=\sum_{n=0}^\infty r_n z^n\\
&=r_0 z^0 + r_1 z^1 + r_2 z^2 + r_3 z^3 + \sum_{n=4}^\infty r_n z^n\\
&=p^3 z^3 + \sum_{n=4}^\infty (q r_{n-1}+pq r_{n-2} + p^2 q r_{n-3}) z^n\\
&=p^3 z^3 + q z \sum_{n=4}^\infty r_{n-1}z^{n-1} + pq z^2 \sum_{n=4}^\infty  r_{n-2} z^{n-2} + p^2q z^3 \sum_{n=4}^\infty r_{n-3}z^{n-3}\\
&=p^3 z^3 + q z \sum_{n=3}^\infty r_{n}z^{n} + pq z^2 \sum_{n=2}^\infty  r_{n} z^{n} + p^2q z^3 \sum_{n=1}^\infty r_{n}z^{n}\\
&=p^3 z^3 + q z g(z) + pq z^2 g(z) + p^2q z^3 g(z)\\
\left( 1-qz-pqz^2-p^2qz^3 \right)g(z)
&=p^3 z^3\\
g(z)
&=\frac{p^3 z^3}{1-qz-pqz^2-p^2qz^3}
\end{aligned}
$$

And $g^{(k)}(1)=\mathbb{E}\left(\prod_{i=0}^{k-1}(X-i)\right)$ use Stirling numbers to convert $x^{\underline k}$ to $x^{k}$,
or use an iterative approach
$\mathbb{E}(X)=g'(1),
\mathbb{E}(X^2)={\left[ \left(z g' \right)' \right]}_{z=1},
\mathbb{E}(X^3)={\left[ \left(z \left( zg' \right)' \right)' \right]}_{z=1},
$.

**Alternative solution:** Markov chain (KMP automata + transition probability graph)

### order statistics of i.i.d continuous random variables

Let $X_1,X_2\ldots X_n$ be $n$ i.i.d. continuous r.v.s whose CDF is $F(x)$.  
In a random experiment, the result is $S=(x_1,x_2\ldots x_n)$, let $X_{(k)}$ be the $k$-th smallest value in $S$.

#### CDF

Then the CDF of $X_{(k)}$ is

$$
F_{X_{(k)}}(x)=P(X_{(k)}\leq x)=\sum_{i=k}^n \binom{n}{i} {(F(x))}^i {(1-F(x))}^{n-i}
$$

For any $x$, consider the random variable $Y=\sum_{i=1}^n \left[ X_i\leq x \right]$,
then $Y\sim\mathrm{Bin}\left(n,F(x)\right)$.  
If $X_{k}\leq x$ then there are at least $k$ variable in $X_1\ldots X_n$ such that $X_i\leq x$,  
therefore $P(X_{k}\leq x)=P(Y\geq k)$.

#### (marginal) PDF

Then, let's find the PDF of $X_{(k)}$, let $f_k(x)$ be the PDF,  
then for a small interval $\delta$, we have$P(x-\delta<X_{(k)}<x+\delta)=2\delta f_{k}(x)$.  

$$
\begin{aligned}
2\delta f_k(x)
&=P(x-\delta<X_{(k)}<x+\delta)\\
\text{(LOTP. Which $X_i$ becomes the $k$-th)}
&=\sum_{i=1}^n P(x-\delta < X_{(k)} < x+\delta | X_{(k)}=X_i)P(X_{(k)}=X_i)\\
\text{(i.i.d. continuous random variable symmetry)}
&=n P(x-\delta < X_{(k)} < x+\delta | X_{(k)}=X_1)P(X_{(k)}=X_1)\\
\text{(i.i.d. continuous random variable symmetry)}
&=P(x-\delta < X_{(k)} < x+\delta | X_{(k)}=X_1)\\
\text{(by definition)}
&=\frac{ P(x-\delta < X_{(k)} < x+\delta ,\ X_{(k)}=X_1) }{P(X_{(k)}=X_1)} \\
\text{($\ast$)}
&=\frac{ 2\delta f_{X_1}(x)\ \binom{n-1}{k-1}{(F(x))}^{k-1}{(1-F(x))}^{n-k} }{1/n} \\
&=n\ 2\delta f(x)\ \binom{n-1}{k-1}{(F(x))}^{k-1}{(1-F(x))}^{n-k}\\
f_{X_{(k)}}(x)
&=nf(x) \binom{n-1}{k-1}{(F(x))}^{k-1}{(1-F(x))}^{n-k}\\
\end{aligned}
$$

Consider the $(\ast)$ step. $P(x-\delta < X_{(k)} < x+\delta ,\ X_{(k)}=X_1)$.
To make that happen, both of the follwing two properties have to be satisfied.

- Place $X_1$ in the range $(x-\delta,x+\delta)$.  
- For $X_2,X_3\ldots X_n$. Select $(k-1)$ $X_j$s that fall on to the left of $x$, while the other $(n-k)$ $X_j$s fall on to the right of $x$.

Since all $X_j$s are independent, the probability is

$$
2\delta f(x) \ \binom{n-1}{k-1}{(F(x))}^{k-1}{(1-F(x))}^{n-k}
$$

#### (joint) PDF

$$
f_{(X_{(1)},X_{(2)}\ldots, X_{(n)})}(x_1,x_2\ldots x_n)
=n! \prod_{i=1}^n f(x_i)
$$

1. For all permutation of $[n]$ $p_1,p_2\ldots p_n$,
   let $X_{(i)}=X_{p_i}$.  
2. Put $X_{p_i}$ at $(x_i-\delta,x_i+\delta)$

Since $X_1,X_2\ldots X_n$ are independent, the probability of the product event is the product of the probability of each event.

#### identity

$$
\begin{aligned}
P(X_{(k)}<x)
&=F_{X_{(k)}}(x)
=\sum_{i=k}^n \binom{n}{i} {(F(x))}^i {(1-F(x))}^{n-i}\\
&=\int_{\infty}^x f_{X_{(k)}}(t)\mathrm{d}{t}
=\int_{\infty}^x n\binom{n-1}{k-1}f(t){F(t)}^{k-1}{(1-F(t))}^{n-k}\mathrm{d}{t}
\end{aligned}
$$

#### ordered statistics of $\mathrm{Unif}(0,1)$

For $n$ i.i.d. $\mathrm{Unif}(0,1)$ r.v.s. $X_1,X_2\ldots X_n$

$$
\begin{aligned}
P(X_{(k)}<x)
&=\sum_{i=k}^n\binom{n}{i}{(F(x))}^i{(1-F(x))}^{n-i}\\
&=\sum_{i=k}^n\binom{n}{i}x^i {(1-x)}^{n-i}\\
&=\int_{0}^{x}n\binom{n-1}{k-1}f(x) {(F(x))}^{i-1} {(1-F(x))}^{n-i}\mathrm{d}t\\
&=\int_{0}^{x}n\binom{n-1}{k-1} x^{i-1} {(1-x)}^{n-i}\mathrm{d}t\\
\end{aligned}
$$

we have a few more related identities, $\sum_{i=k}^n,\sum_{i=0}^k,\int_0^x,\int_x^1$.

#### two indentities

$$
\begin{aligned}
&\forall 0<p<1,\, k\in \mathbb{N}
\qquad
&\sum_{i=0}^k \binom{n}{i}p^i {(1-p)}^{n-i}
=(n-k)\binom{n}{k}\int_p^1 x^k{(1-x)}^{n-k-1}\mathrm{d}x\\
&\forall 0\leq k\leq n,\ k,n\in \mathbb{N}
\qquad
&\int_0^1 \binom{n}{k}x^k{(1-x)}^{n-k}\mathrm{d}x
=\frac{1}{n+1}
\end{aligned}
$$

1. For the first one.  
   Consider the ordered statistics of $n$ i.i.d. $\mathrm{Unif}(0,1)$ r.v.s.  
   This is the probability that $X_{(k+1)}>p$
2. For the second one  
   Consider $n+1$ i.i.d. $\mathrm{Unif}(0,1)$ r.v.s $X_1,X_2\ldots X_n,X_{n+1}$.  
   $P(X_1=X_{(k+1)}| X_1=x)$ is $\binom{n}{k}x^k{(1-x)}^{n-k}$.  
   By LOTP, LHS is $P(X_1=X_{(k+1)})=\frac{1}{n+1}$ i.i.d. continuous r.v.s symmetry property.

### note: abs and min/max

$$
\begin{aligned}
\max(x,y)+\min(x,y)
&=x+y\\
\max(x,y)-\min(x,y)
&=\begin{cases}
x-y & x\geq y\\
y-x & x<y\\
\end{cases}
=|x-y|\\
x+y + |x-y|
&=(\max(x,y)+\min(x,y))+(\max(x,y)-\min(x,y))=\max(x,y)|\\
x+y - |x-y|
&=(\max(x,y)+\min(x,y))-(\max(x,y)-\min(x,y))=\min(x,y)|\\
\end{aligned}
$$

### Beta and Gamma

#### Beta integral and Gamma integral

$$
\begin{aligned}
\beta(a,b)&=\int_0^1 x^{a-1} {(1-x)}^{b-1}\mathrm{d}x & (a,b>0)\\
\Gamma(a) &=\int_0^{+\infty} x^{a-1}e^{-x}\mathrm{d}x=\int_0^{+\infty} x^{a}e^{-x}\frac{\mathrm{d}x}{x} & (a>0)\\
\end{aligned}
$$

Useful properties

- $\beta(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$
- $\Gamma(a+1)=a\Gamma(a)$
- $\Gamma(1)=1,\Gamma(n)=(n-1)!$

#### Beta distribution and Gamma distribution

- $X\sim \mathrm{Beta}(a,b)$ if the PDF of $X$ is $f(x)=\frac{1}{\beta(a,b)} x^{a-1}{(1-x)}^{b-1}$ for $0<x<1$
- $X\sim \mathrm{Gamma}(a,1)$ if the PDF of $X$ is $f(x)=\frac{1}{\Gamma(a)} x^{a-1}e^{-x}=\frac{1}{\Gamma(a)}x^a e^{-x} \frac{1}{x}$ for $x>0$

if $X\sim \mathrm{Beta}(a,b)$ then, $Y=1-x\sim\mathrm{Beta}(b,a)$

$$
\begin{aligned}
\mathbb{E}(X)
&=\frac{1}{\beta(a,b)}\int_0^1 x\cdot x^{a-1}{(1-x)}^{b-1}\mathrm{d}x
=\frac{1}{\beta(a,b)}\int_0^1 x^{a}{(1-x)}^{b-1}\mathrm{d}x\\
&=\frac{\beta(a+1,b)}{\beta(a,b)}
=\frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)}\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}
=\frac{\Gamma(a+1)}{\Gamma(a)}\frac{\Gamma(a+b)}{\Gamma(a+b+1)}\\
&=\frac{a}{a+b}\\
f_Y(y)&=f_x(x)\begin{vmatrix} \frac{\mathrm{d}x}{\mathrm{d}y} \end{vmatrix}=f_x(1-y)|-1|\\
&=\frac{1}{\beta(a,b)} {(1-y)}^{a-1} y^b
=\frac{1}{\beta(b,a)} y^b {(1-y)}^{a-1}
\end{aligned}
$$

If $X\sim\mathrm{Gamma}(a,1)$ then $Y=\frac{X}{\lambda}\sim\mathrm{Gamma}(a,\lambda)$

$$
\begin{aligned}
f_Y(y)
&=f_X(x)\frac{\mathrm{d} x}{\mathrm{d} y}
=\frac{1}{\Gamma(a)} {(\lambda y)}^{a-1}e^{-\lambda y}\cdot \lambda
=\frac{1}{\Gamma(a)} {(\lambda y)}^{a} e^{-\lambda y}\frac{1}{y}\\
\mathbb{E}(X)
&=\int_0^{+\infty}\frac{x}{\Gamma(a)}x^{a-1}e^{-x}\mathrm{d}x
=\int_0^{+\infty}\frac{1}{\Gamma(a)}x^{a}e^{-x}\mathrm{d}x
=\frac{\Gamma(a+1)}{\Gamma(a)}
=a\\
\mathbb{E}(X^2)
&=\int_0^{+\infty}\frac{1}{\Gamma(a)}x^{a+1}e^{-x}\mathrm{d}x
=\frac{\Gamma(a+2)}{\Gamma(a)}
=a(a+1)\\
\mathbb{E}(X^k)
&=\int_0^{+\infty}\frac{1}{\Gamma(a)}x^{a+1}e^{-x}\mathrm{d}x
=\frac{\Gamma(a+k)}{\Gamma(a)}
=a^{\overline k}\\
\mathrm{Var}(X)&=\mathbb{E}(X)=a
\end{aligned}
$$

#### Gamma, sum of independent Exponentials, arrival times in Poisson process

- For $n$ i.i.d. $\mathrm{Expo}(\lambda)$ r.v.s. $X_1,X_2\ldots X_n$,
the sum $Y=\sum_{i=1}^n X_i$ have a $\mathrm{Gamma}(n,\lambda)$ distribution.
- In a Poisson process with prameter $\lambda$, the $k$-th arrival time $T_k$ has $\mathrm{Gamma}(k,\lambda)$ distribution.  
  (note: $T_1<T_2\ldots T_n$, are dependent.)

#### Beta-Gamma connection

If $X\sim \mathrm{Gamma}(a,\lambda),Y\sim \mathrm{Gamma}(b,\lambda)$ are independent.  
Let $T=X+Y, W=\frac{X}{X+Y}$, then $T\sim \mathrm{Gamma}(a+b,\lambda),W\sim\mathrm{Beta}(a,b)$ are independent.

$$
\begin{aligned}
(x,y)&=(tw,t(1-w)) \quad (t,w) = (x+y,\frac{x}{x+y})\\
\begin{vmatrix}
\frac{\partial (x,y)}{\partial (t,w)}
\end{vmatrix}
&=\begin{vmatrix}
w   & t\\
1-w &  -t
\end{vmatrix}
=-wt-t(1-w)=-t
\end{aligned}
$$

$$
\begin{aligned}
f_{T,W}(t,w)
&=f_{X,Y}(x,y)
\begin{Vmatrix}
\frac{\partial (x,y)}{\partial (t,w)}
\end{Vmatrix}
=f_{X}(x)f_{Y}(y)
\begin{Vmatrix}
\frac{\partial (x,y)}{\partial (t,w)}
\end{Vmatrix}\\
&=
\frac{1}{\Gamma(a)} {(\lambda x)}^a e^{-\lambda x} \frac{1}{x}
\cdot
\frac{1}{\Gamma(b)} {(\lambda y)}^b e^{-\lambda y} \frac{1}{y}
\cdot t\\
&=
\frac{1}{\Gamma(a)} {(\lambda tw)}^a e^{-\lambda tw} \frac{1}{tw}
\cdot
\frac{1}{\Gamma(b)} {(\lambda t(1-w))}^b e^{-\lambda t(1-w)} \frac{1}{t(1-w)}
\cdot t\\
&=
\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} w^{a-1} {(1-w)}^{b-1}
\cdot
\frac{1}{\Gamma(a+b)} {(\lambda t)}^{a+b}e^{-\lambda t} \frac{1}{t}\\
\implies
f_{T,W}(t,w)&=f_T(t)f_W(w)
\quad
\begin{cases}
f_T(t)&=\frac{1}{\Gamma(a+b)} {(\lambda t)}^{a+b}e^{-\lambda t} \frac{1}{t}\\
f_W(w)&=\frac{1}{\beta(a,b)}w^{a-1}{(1-w)}^{b-1}\\
\end{cases}
\end{aligned}
$$

#### Story of Beta and Gamma distribution

- **Beta: ordered statistics of uniforms**  
  $\beta(a+1,b+1)=\int_0^1 x^a{(1-x)}^b\mathrm{d}x=\frac{\Gamma(a+1)\Gamma(b+1)}{\Gamma(a+b+2)}=\frac{a! b!}{(a+b+1)!}$  
  Suppose that $X_1\ldots X_a, Z, Y_1\ldots Y_b$ are i.i.d. $\mathrm{Unif}(0,1)$ r.v.s.  
  The probability that $\max(X_1\ldots X_a) < Z < \min(Y_1\ldots Y_b)$ is $x^a {(1-x)}^b$
- **Beta: (unknown) success probability of Bernoulli trials**  
  In $\mathrm{Beta}(a,b)$, the parameters $a,b$ are called pesudo counts, where $a$ is the number of successful trials and $b$ is the number of failed trails.  
  If $p\sim \mathrm{Beta}(a,b), X|p\sim \mathrm{Bin}(n,p)$ and $X=k$ is observed.  
  The distribution of success probability get updated to $\mathrm{Beta}(a+k,b+(n-k))$
- **Gamma: waiting time until the n-th arrival**  
  $\mathrm{Gamma}(1,\lambda)=\mathrm{Expo}(\lambda)$  
  If $X_1\ldots X_n\sim\mathrm{Expo}(\lambda)$ are i.i.d. r.v.s.
  Then $\sum_i X_i\sim\mathrm{Gamma}(n,\lambda)$.
- **Beta: the fraction of waiting time**  
  $X\sim \mathrm{Gamma}(a,\lambda), Y\sim \mathrm{Gamma}(b,\lambda)$, waiting for $a+b$ independent arrivals with rate $\lambda$  
  Then $\frac{X}{X+Y}\sim\mathrm{Beta}(a,b)$
- **Gamma: (unknown) rate of Poisson process**  
  $\lambda \sim \mathrm{Gamma}(r,b)$ is the prior distribution of the arrival rate $\lambda$  
  Then $X|\lambda \sim\mathrm{Pois}(\lambda t)$, the number of arrivals in $[0,t]$ for a Poisson process with parameter $\lambda$.  
  If $X=n$ is observed, the posterior distribution of $\lambda$ is updated to $\mathrm{Gamma}(r+n,b+t)$  
  $r$ is the number of arrivals, $b$ is the total waiting time. The average rate is $\mathbb{E}(\lambda)=\left(\frac{r}{b}\to \frac{r+n}{b+t}\right)$

### Conjugacy

reference: [wikipedia: conjugacy prior](https://en.wikipedia.org/wiki/Conjugate_prior)

#### Beta-Binomial

If $p$ have a $\mathrm{Beta}(a,b)$ prior distribution, and $X|p=\mathrm{Bin}(n,p)$.  
If $X=k$ is observed, the posterior distribution of $p$, $p|X=k$ is $\mathrm{Beta}(a+k,b+n-k)$  
The positive real number $a,b$ are called _pesudo count_.  

The prior distribution can be history records, uniform, or arbitarily picked things.

$$
\begin{aligned}
f_{p|X=k}(t,k)
&=\frac{P(X=k|p=t)f_p(t)}{P(X=k)}\\
&=\frac{
\binom{n}{k} t^k {(1-t)}^{n-k}
\,
\frac{1}{\beta(a,b)} t^{a-1}{(1-t)}^{b-1}
}{P(X=k)}\\
&=\frac{\binom{n}{k}}{\beta(a,b) P(X=k)} t^{a+k-1} {(1-t)}^{b+n-k-1}
\end{aligned}
$$

The conditional PDF is also a valid PDF which integrates to $1$, thus $\frac{\binom{n}{k}}{\beta(a,b) P(X=k)}=\beta(a+k,b+n-k)$.
The posterior distribution is $\mathrm{Beta}(a+k,b+n-k)$

note: if a $\mathrm{Unif}(0,1)$ prior, we can use $\mathrm{Beta}(1,1)$.

#### Dirichlet-Multinomial

An extension of the Beta-Binomial conjugacy.

- prior distribution: $\mathrm{Dirichlet}(a_1,a_2\ldots a_k)$  
  $f(p_1,p_2\ldots p_k)=\frac{\Gamma\left(\sum_i a_i\right)}{\prod_i \Gamma(a_i)}\prod_i p_i^{a_i}$
- observation: $(X_1,X_2\ldots X_k)|(p_1,p_2\ldots p_k) \sim \mathrm{Multi}(n,(p_1,p_2\ldots p_k))$  
  $P(X_1=n_1\ldots X_k=n_k)=\frac{n!}{\prod_i n_i!} p_i^{x_i}$ where $\sum_i n_i=n$
- prior distribution: $\mathrm{Dirichlet}(a_1+X_1,a_2+X_2\ldots a_k+X_k)$  
  $f(\vec p|\vec X)$

#### Gamma-Poisson

- The prior distribution of $\lambda$ is $\mathrm{Gamma}(r,b)$  
- Obersvation $X|\lambda\sim \mathrm{Pois}(\lambda t)$. The number of arrivals in a Poisson process with parameter $\lambda$ in $[0,t]$ time.
- The posterior distribution $\lambda|X \sim \mathrm{Gamma}(r+n,b+t)$

$$
\begin{aligned}
f_{\lambda|X}(\lambda|X=n)
&=\frac{P(X=n|\lambda)f(\lambda)}{P(X=n)}\\
&=\frac{
e^{-\lambda t}\frac{{(\lambda t)}^n}{n!}
\cdot
\frac{1}{\Gamma(r)} {(b\lambda)}^{r} e^{-b\lambda}\frac{1}{\lambda}
}{P(X=n)}\\
&=\frac{\Gamma(r+n) b^r}{\Gamma(r)n!P(X=n) {(b+t)}^{r+n}}
\cdot
\frac{1}{\Gamma(r+n)} {((b+t)\lambda)}^{r+n} e^{-(b+t)\lambda} \frac{1}{\lambda}\\
&\propto
\frac{1}{\Gamma(r+n)} {((b+t)\lambda)}^{r+n} e^{-(b+t)\lambda} \frac{1}{\lambda}
\end{aligned}
$$

Other factors are independent of $\lambda$, so they can be viewed as normalizing constants.  
Thus, the PDF is

$$
f_{\lambda|X}(\lambda|X=n)
=\frac{1}{\Gamma(r+n)} {((b+t)\lambda)}^{r+n} e^{-(b+t)\lambda} \frac{1}{\lambda}
$$

Which is $\mathrm{Gamma}(n+r,b+t)$

$$
\frac{r}{b}
\to
\frac{r+n}{b+t}
$$

### comparison: MAP,MLE,Bayesian Mean

- MAP: Bayesian statistical inference. Maximize the posterior probability
- MLE: Classical statistical inference. Maximize the likehood function.
- Bayesian Mean: Bayesian statistical inference. Conditional Expectation.
