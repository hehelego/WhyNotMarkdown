# 概率论与数理统计 note

> textbook: Introduction to Probability
> author: Joseph K. Blitzstein, Jessica Hwang
> course: Harvard STAT110 Introduction to Probability
>
> textbook: Introduction to Probability
> author: Dimitri P. Bertsekas, John N. Tsitsiklis
> course: MIT 6-041: Probabilistic Systems Analysis & Applied Probability

[TOC]

## ch0 Introduction

## ch1 Probability and Counting

### birthday problem

$$
\begin{aligned}
p_{k,n}
&=\prod_{i=0}^{k-1}\frac{n-i}{n}
=\prod_{i=0}^{k-1}\left(1 - \frac{i}{n} \right)\\
e^x &\approx x+1\quad (|x|\to 0)\\
p_{n,k}
&\approx \prod_{i=0}^{k-1} e^{-\frac{i}{n}}
=\prod_{i=0}^{k-1} \exp\left( {-\frac{i}{n}} \right)
=\exp\left( {-\frac{k(k-1)}{n}} \right)
\end{aligned}
$$

Find the relation between n and k when $p_{n,k}\leq \frac{1}{2}$, using the approximation.  

$$
\begin{aligned}
&p_{n,k}
\approx \exp\left( -\frac{k(k-1)}{n} \right)
\approx \exp\left( -\frac{k^2}{n} \right)
\leq \frac{1}{2}\\
&-\frac{k^2}{n} \leq -\ln 2\quad k\geq \sqrt{n\ln 2}
\end{aligned}
$$

### probability axioms

$(P,S)$ probability space: probability function and probability sample space.

- $\forall A\subseteq S\quad 0\leq P(A)\leq 1$
- $P(\varnothing)=0,\ P(S)=1$
- $A_1,A_2\ldots A_n\ldots$ are disjoint subsets of $S$  
  $P\left(\bigcup_{i=1}^\infty A_i\right)=\sum_{i=1}^\infty P(A_i)$

#### deriving properties of probability from the axioms

complement rule:  
$A\cap A^c = \varnothing, A\cup A^c = S$, so $P(A\cup A^c)=P(A)+P(A^c)=P(S)=1$,  
thus $P(A)=1-P(A^c)$

inclusive-exclusive principle:  
$A\setminus (A\cap B), B$ is a partition of $A\cup B$. $P(A\cup B)=P(A\setminus (A\cap B))+P(B)$  
$A\setminus (A\cap B), A\cap B$ is a partition of $A$. $P(A)=P(A\setminus (A\cap B))+P(A\cap B)$  
$P(A\cup B)=P(A)+P(B)-P(A\cap B)$

subset monotonous:  
$A\subseteq B$.  
$B\setminus A,A$ is a partition of $B$, so $P(B)=P(A)+P(B\setminus A)\geq P(A)+0=P(A)$

Boole inequality: $P(\bigcup_{i=1}^\infty A_i)\leq \sum_{i=1}^\infty A_i$  
Let $B_1=A_1,B_2=A_2\cap \overline{A_1},B_3=A_3\cap \overline{A_1\cap A_2}\ldots B_n=A_n\cap \overline{\bigcap_{i=1}^{n-1} A_i}\ldots$  
The sets $B_i$ are disjoint and $\forall n \bigcup_{i=1}^n A_i=\bigcup_{i=1}^n B_i$  
$$
P\left(\bigcup_{i=1}^\infty A_i\right)
= P\left(\bigcup_{i=1}^\infty B_i\right)
= \sum_{i=1}^n P(B_i)
\leq \sum_{i=1}^n P(A_i)
$$

## ch2 Conditional Probability

LOTP, LOTE (law of total probability/expectation): let $(A_1,A_2\ldots A_n)$ be a partition of $S$  
partition: $\bigcup A_i=S$, $i\neq j\rightarrow A_i\cap A_j=\varnothing$

$$
\begin{aligned}
P(B) &= \sum_{i=1}^n P(B | A_i) P(A_i)\\
E(B) &= \sum_{i=1}^n E(B | A_i) P(A_i)
\end{aligned}
$$

Bayes's rule:

$$
P(A|B)
=\frac{P(B|A)P(A)}{P(B)}
=\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B| A^c)P(A^c)}
$$

### conditional probability as a probability function

In a probability space $(S,P)$ and an event $C\subseteq S$.  
Define the function $\hat P(A)=P(A\mid C)$,
then $(S,\hat P)$ consists a probability space.

- Bayes' rule: $\hat P(A|B)=\frac{\hat P(B|A)P(A)}{P(B)}$  
  which is $P(A\mid B,C)=\frac{P(B | A,C)P(A | C)}{P(B|C)}$
- complement: $\hat P(A^c)=1-\hat P(A)$
- inclusive-exclusive: $\hat P(A\cup B)=\hat P(A)+\hat P(B)-\hat P(A\cap B)$

### independence

Definition: two events $A,B$ are said to be independent if "prior probability equals to posterior probability"

$$
P(A\cap B)=P(A)P(B)
\quad
P(A\mid B)=\frac{P(AB)}{P(B)}=P(A)
$$

the following statements are equivalent

- $A$ and $B$ are independent
- $A^c$ and $B$ are independent
- $A$ and $B^c$ are independent
- $A^c$ and $B^c$ are independent

#### comparison: pairwise independence and independence

PID doesn't imply ID, on the contrary ID implies PID.

e.g. Consider tossing two independent fair coin.

- A: the first coin lands on head.
- B: the second coin lands on head.
- C: the two coin give the same result.

A, B, C are pairwise independent but not independent.  
($A\cap B$ implies $C$)

$$
P(A,B,C)=\frac{1}{4}\neq P(A)P(B)P(C)=\frac{1}{8}
$$

### conditional independence

A, B are said to be conditional independent given $E$ if.
$$
P(A,B | E) = P(A|E)P(B|E)
$$

#### comparison: conditional independence and independence

##### CID can't imply ID

Given a fair coin and a biased coin (land on head at 1/2, 3/4 probability respectively)  
We choose a coin randomly and flip it twice.

- F: the fair coin is chosen
- A: the first tosses landing heads
- B: the second tosses landing heads

$A, B$ are conditional independent given $F$.  

$$
P(A,B | F)=\frac{1}{2}\frac{1}{2} = P(A|F)P(B|F)
$$

But $A,B$ are not independent.

$$
\begin{aligned}
P(A) &= P(A | F)P(F) + P(A | F^c)P(F^c) = \frac{1}{2}\frac{1}{2} + \frac{3}{4}\frac{1}{2} = \frac{5}{8}\\
P(B) &= P(A | F)P(F) + P(A | F^c)P(F^c) = \frac{1}{2}\frac{1}{2} + \frac{3}{4}\frac{1}{2} = \frac{5}{8}\\
P(AB)
&= P(AB | F)P(F) + P(AB | F^c)P(F^c)
=\frac{1}{2} {\left( \frac{1}{2} \right)}^2
+\frac{1}{2} {\left( \frac{3}{4} \right)}^2
=\frac{13}{32}\\
P(A)P(B) &= \frac{25}{64}
\end{aligned}
$$

##### ID can't imply CID

Given two independent events $A, B$ where $0<P(A)=p,P(B)=q<1$, let $C=A\bar{B}+\bar{A}B$.  
$A, \bar{B}$ are not are conditional independent given $C$.

$$
\begin{aligned}
P(A|C) &= \frac{P(A\cap C)}{P(C)}=\frac{P(A\bar{B})}{P(A\bar{B}+\bar{A}B)}=\frac{p(1-q)}{p(1-q)+q(1-p)}>0\\
P(B|C) &= \frac{P(B\cap C)}{P(C)}=\frac{q(1-p)}{p(1-q)+q(1-p)}>0\\
P(A\cap B|C) &= 0\\
P(A|C)P(B|C) &> 0
\end{aligned}
$$

##### CID given E v.s. CID given complement of E

e.g. The courses in university are either graded base on effort or graded base on random number generator.

- G: a course grades students according to academical performance.
- W: work hard.
- A: receive A plus grade.

$A, W$ are conditional independent given $G^c$, but are related(not independent) given $G$.

### example: distribution of the sum of independent identical dice roll

> source:
>
> - Introduction to probability
> - Joseph K. Blitzstein, Jessica Hwang
> - second edition
> - chapter2, exercise 48
>
> [see also](https://www.zhihu.com/question/336062847) `从0点出发,每次向正方向随机走1~k步,求踩到x的概率. 为什么x趋于无穷大时,概率为2/(k+1)`

#### problem statement

A fair die is rolled repeatedly, and a running total is kept (which is, at each time, the total of all the rolls up until that time). Let $p_n$ be the probability that the running total is ever exactly $n$ (assume the die will always be rolled enough time so that the running total will eventually exceed $n$, but it may or may not ever equal $n$).

- write down a recursive relation for $p_n$.
- find $\lim_{n\to \infty}p_n$

#### solution

the RR

$$
p_n=
\begin{cases}
0& n<0\\
1& n=1\\
\frac{1}{6}\left(\sum_{i=1}^6 p_{n-i}\right) & n>1\\
\end{cases}
$$

Every time we roll a die, the sum increase by $\frac{\sum_{i=1}^6 i}{6}=\frac{7}{2}$ on average.
It can be viewed/interpreted in this way "in every 7 consecutive numbers the sum lands on 2 of the 7 numbers", so the probability converges to $\frac{2}{7}$.

A somewhat more convincing proof

$$
\begin{aligned}
 &p_{n+1}+2p_{n+2}+3p_{n+3}+4p_{n+4}+5p_{n+5}+6p_{n+6}\\
=&p_{n+1}+2p_{n+2}+3p_{n+3}+4p_{n+4}+5p_{n+5}+(p_n+p_{n+1}+2p_{n+2}+3p_{n+3}+4p_{n+4}+5p_{n+5})\\
=&p_n+2p_{n+1}+3p_{n+2}+4p_{n+3}+5p_{n+4}+6p_{n+5}\\
=&\cdots\\
=&p_{-5}+2p_{-4}+3p_{-3}+4p_{-2}+5p_{-1}+6p_{0}\\
=&6
\end{aligned}
$$

taking limit when $n\to \infty$ on both side, $6=\sum_{i=1}^6 i\lim_{n\to\infty}p_n$, so $p\to \frac{2}{7}$

## ch3 Random Variables

## ch4 Expectation

### more on indicators

For $n$ (arbitary, independence is not required)events $A_1,A_2\ldots A_n$ and their indicators $I_1,I_2\ldots I_n$.  
Let $X=\sum_{i=1}^n I_i$, consider $\binom{X}{k}$

- $k=0$, $\binom{X}{0}=1$ and $E\left(\binom{X}{0}\right)=1$
- $k=1$, $\binom{X}{1}=X$ and $E\left(\binom{X}{1}\right)=X$
- $k=2$, $\binom{X}{2}=\frac{X(X-1)}{2}=\sum_{i<j}I_iI_j$ and $E\left(\binom{X}{2}\right)=\sum_{i<j}P(A_i\cap A_j)$  
  $
  \begin{aligned}
  X(X-1)
  &=X^2-X\\
  &=\left(2\sum_{i<j}I_iI_j+\sum_i I_i^2\right)-\sum_i I_i\\
  &=\left(2\sum_{i<j}I_iI_j+\sum_i I_i\right)-\sum_i I_i\\
  &=2\sum_{i<j}I_iI_j
  \end{aligned}
  $
- $k=3$, $\binom{X}{3}=\frac{X(X-1)(X-2)}{6}=\sum_{i<j<k}I_iI_jI_k$ and $E\left(\binom{X}{3}\right)=\sum_{i<j<k}P(A_i\cap A_j\cap A_k)$  
  $
  \begin{aligned}
  X(X-1)(X-2)
  &=\left(2\sum_{i<j}I_iI_j\right)\left(\sum_i I_k - 2\right)\\
  &=2\left(\sum_{i<j\land k=i}I_iI_jI_k+\sum_{i<j\land k\neq i\land k\neq j}I_iI_jI_k+\sum_{i<j\land k=j}I_iI_jI_k-2\sum_{i<j}I_iI_j\right)\\
  &=2\left(\sum_{i<j\land k\neq i\land k\neq j}I_iI_jI_k\right)\\
  &=2\left(\sum_{p<q<r}I_pI_qI_r\sum_{i<j\land k\neq i\land k\neq j}[(p,q,r)\in \{(i,j,k),(i,k,j),(k,i,j)\}]\right)\\
  &=2\left(3\sum_{i<j<k}I_iI_jI_k\right)=6\sum_{i<j<k}I_iI_jI_k
  \end{aligned}
  $
- $k=m$, $\binom{X}{m}=\sum_{i_1<i_2\ldots i_m}\prod_{j=1}^m I_j$ and $E\left(\binom{X}{m}\right)=\sum_{i_1<i_2\ldots i_m}P(\prod_{j=1}^m A_{i_j})$

#### evaluate moments of binomial distribution

Given that $X\sim \mathrm{Bin}(n,p)$.  
Let $A_j$ be the event that the $j$th Bernoulli trial success and $I_j$ be the corresponding indicator.  

$
\begin{aligned}
\binom{X}{k}
&=\sum_{p_1<p_2\ldots p_k}\sum_{i=1}^k I_{p_i}\\
\mathbb{E}\binom{X}{k}
&=\sum_{p_1<p_2\ldots p_k}P\left(\prod_{i=1}^k A_{p_i}\right)
=\sum_{p_1<p_2\ldots p_k}p^k=\binom{n}{k}p^k\\
X^m
&=\sum_{i=1}^m \binom{X}{i}i!\begin{Bmatrix}m\\ i\end{Bmatrix}\\
\mathbb{E}(X^m)
&=\sum_{i=1}^m \binom{n}{i}p^i i!\begin{Bmatrix}m\\ i\end{Bmatrix}
=\sum_{i=1}^m n^{\underline i}p^i\begin{Bmatrix}m\\ i\end{Bmatrix}\\
\end{aligned}
$

## ch5 Continuous Random Variable

### normals

If $
X\sim \mathcal{N}(\mu_1,\sigma_1^2),
Y\sim \mathcal{N}(\mu_2,\sigma_2^2)
$ are independent,
then $X+Y\sim \mathcal{N}(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$.

Consider the MGF of standard normal $Z\sim \mathcal{N}(0,1)$

$$
\begin{aligned}
M_Z(t)
&
=\mathbb{E}(e^{tZ})
=\int_{-\infty}^{+\infty} e^{tz}\, \frac{1}{\sqrt{2\pi}}e^{-z^2/2}\mathrm{d}z\\
&
=\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(z^2-2tz)}\mathrm{d}z
=\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}{(z-t)}^2+\frac{1}{2}t^2}\mathrm{d}z\\
&
=e^{\frac{1}{2}t^2}\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}{(z-t)}^2}\mathrm{d}z\\
&
=e^{\frac{1}{2}t^2}\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2}\mathrm{d}z
=e^{\frac{1}{2}t^2}\\
&
=\sum_{n=0}^\infty \frac{{(t^2/2)}^n}{n!}
=\sum_{n=0}^\infty \frac{1}{2^n n!}t^{2n}
=\sum_{n=0}^\infty \frac{(2n)!}{2^n\, n!} \frac{t^{2n}}{(2n)!}
=\sum_{n=0}^\infty \left(\prod_{j=1}^{n}(2j-1)\right)\frac{t^{2n}}{(2n)!}
\end{aligned}
$$

Then $M_X(t)=\mathbb{E}(e^{(\sigma Z+\mu)t})=e^{\mu t}\mathbb{E}(e^{(\sigma t)Z})=e^{\mu t}M_{Z}(\sigma t)=e^{\mu t+\frac{\sigma^2}{2}t^2}$  
For $M_{X+Y}(t)=\mathbb{E}(e^{(X+Y)t})=\mathbb{E}(e^Xe^Y)$,
since the two r.v.s are independent, the expectation of product if the product of expectation,
$M_{X+Y}(t)=e^{\mu_1 t+\frac{\sigma_1^2}{2}t^2}e^{\mu_2 t+\frac{\sigma_2^2}{2} t^2}=e^{(\mu_1+\mu_2)t + \frac{\sigma_1^2+\sigma_2^2}{2}t}$.  
This is the MGF of $\mathcal{N}(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$, which shows that $X+Y\sim \mathcal{N}(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$

### distance between two independent identical normals

$X,Y\sim\mathcal{N}(\mu,\sigma^2)$ are independent, evaluate $\mathbb{E}|X-Y|$.  
Consider $X'=X-\mu,Y'=\mu -Y$, then $X',Y'$ are independent $\mathcal{N}(0,\sigma^2)$ r.v.s.  
Then $X-Y=X'+Y'$ has a $\mathcal{N}(0,2\sigma^2)$ distribution,  
which is same to the distribution of $(\sqrt 2 \sigma)Z$ where $Z\sim\mathcal{N}(0,1)$

$$
\begin{aligned}
\mathbb{E}|X-Y|
=\sqrt 2\sigma\ \mathbb{E}(Z)
&=\sqrt 2\sigma\ \int_{-\infty}^{+\infty}|z|\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2}\mathrm{d}z
&=\frac{2\sigma}{\sqrt \pi}\ \int_{0}^{+\infty}ze^{-\frac{1}{2}z^2}\mathrm{d}z\\
&=\frac{2\sigma}{\sqrt \pi}\ \int_{+\infty}^{0} \mathrm{d}\left( e^{-\frac{1}{2}z^2} \right)\\
&=\frac{2\sigma}{\sqrt \pi}
\end{aligned}
$$

### exponentials

$X\sim \mathrm{Expo}(\lambda)$ for $\lambda>0$

$$
\begin{aligned}
f(x)
&=\lambda e^{-\lambda x}
\quad (x>0)\\
F(x)
&=P(X\leq x)
=1-e^{-\lambda x}
\quad (x>0)\\
G(x)
&=P(X>x)
=e^{-\lambda x}
\quad (x>0)
\end{aligned}
$$

Consider $Y=\lambda X$ or $X=\frac{Y}{\lambda}$, we have $Y\sim \mathrm{Expo}(1)$

$$
\begin{aligned}
G_Y(y)=P(Y> y)
&=P(X> \frac{y}{\lambda})
=e^{-\lambda \frac{y}{\lambda}}
=e^{-y}
\quad(y>0)
\end{aligned}
$$

The MGF of $Y$ is

$$
M_Y(t)
=\mathbb{E}(e^{tY})
=\int_0^{+\infty} e^{ty} e^{-y}\mathrm{d}x
=\int_0^{+\infty} e^{(t-1)y}\mathrm{d}x
=\frac{1}{1-t}
\quad (t>0)
$$

The momnets are $\mathbb{E}(Y^n)=n!$ and $\mathbb{E}(X^n)=\mathbb{E}\left( {\left(\frac{Y}{\lambda}\right)}^n \right)=\frac{n!}{\lambda^n}$

### min,max of independent exponentials

Given three independent exponentials random variable.

$
X_1\sim \mathrm{Expo}(\lambda_1),
X_2\sim \mathrm{Expo}(\lambda_2),
X_3\sim \mathrm{Expo}(\lambda_3)
$

$$
\begin{aligned}
P(\min(X_1,X_2,X_3)>t)
&
=P(X_1>t,X_2>t,X_3>t)
=P(X_1>t)P(X_2>t)P(X_3>t)\\
&
=e^{-\lambda_1 t}e^{-\lambda_2 t}e^{-\lambda_3 t}
=e^{-(\lambda_1+\lambda_2+\lambda_3) t}\\
P(\max(X_1,X_2,X_3)<t)
&
=P(X_1<t,X_2<t,X_3<t)
=P(X_1<t)P(X_2<t)P(X_3<t)\\
&
=(1-e^{-\lambda_1 t})(1-e^{-\lambda_2 t})(1-e^{-\lambda_3 t})
\end{aligned}
$$

Thus, $\min(X_1,X_2,X_3)\sim \mathrm{Expo}(\lambda_1+\lambda_2+\lambda_3)$, but $\max(X_1,X_2,X_3)$ is not an exponentials r.v.  

### expectation of max of i.i.d. exponentials via memoryless property

Let $X_1,X_2\ldots X_n\sim\mathrm{Expo}(\lambda)$ be $n$ i.i.d. r.v.s. Find $\mathbb{E}(\max(X_1,X_2\ldots X_n))$  
Let $T_1=\mathrm{kth}(1,\{X_i\}), T_2=\mathrm{kth}(2,\{X_i\})\ldots , T_n=\mathrm{kth}(n,\{X_i\})$ then $\max(\{X_i\})=\sum_i T_i$  

- For $T_1$ can be seen as the first arival time in a Poisson process with parameter $n\lambda$.  
  (merged $n$ Poisson process with parameter $\lambda$)
- For $T_2$, by memoryless property, can be viewed as the first arival time in a Poisson process with parameter $(n-1)\lambda$.  
  (merged $n-1$ Poisson process with parameter $\lambda$)
- For $T_n$, by memoryless property, can be viewed as the first arival time in a Poisson process with parameter $1\lambda$.  
  (a Poisson process with parameter $\lambda$)

Then $\mathbb{E}(\max X_i)=\sum_{i=1}^n \frac{1}{i\lambda}$

## ch6 Moments

### sample variance is an unbiased estimation of the variance

Given $n$ independent identical distributed random variables (the samples) $X_1,X_2\ldots X_n$, where $\mathrm{E}(X_i)=\mu,\mathrm{Var}(X_i)=\sigma^2$
We have the nice relation that $\mathrm{E}\left(\frac{1}{n}\sum_{i=1}^n X_i^k\right)=\frac{1}{n}\sum_{i=1}^n \mathrm{E}(X_i^k)=\frac{1}{n}n\mathrm{E}(X_1^k)=\mathrm{E}(X^n)$ for all $k\in \mathbb{Z}$.  
That is, the expectation of sample moments are equal to the corresponding moments or sample moment is unbiased estimator of moment.

Thus
$$
\begin{aligned}
\mathrm{E}(\overline{X})
&=\mathrm{E}(\frac{1}{n}\sum_{i=1}^n X_i)
=\frac{1}{n}\sum_{i=1}^n \mathrm{E}(X_i)
=\mathrm{E}(X)=\mu\\
\mathrm{Var}(\overline{X})
&=\mathrm{Var}\left(\frac{1}{n}\sum_{i=1}^n X_i\right)
=\frac{1}{n^2}\sum_{i=1}^n \mathrm{Var}(X_i)
=\frac{n\sigma^2}{n^2}
=\frac{\sigma^2}{n}
\end{aligned}
$$

However, we can not find the expectation if the distribution is unknown,
(the only thing that we can get to known is the value of samples in a random experiment)
we have to use other estimator.

We define the **sample variance** and the **sample standard deviation** as follows:
$$
S_n^2=\frac{1}{n-1}\sum_{i=1}^n{\left(X_i-\bar{X}\right)}^2
\quad
S_n=\sqrt{S_n^2}
$$
It turns out that $\mathrm{E}(S_n^2)=\sigma^2$

$$
\begin{aligned}
S_n^2
&=\frac{1}{n-1}\sum_{i=1}^n{\left(X_i-\bar{X}\right)}^2\\
\mathrm{E}\left( (n-1)S_n^2 \right)
&=\mathrm{E} \left( \sum_{i=1}^n{\left(X_i-\bar{X}\right)}^2 \right)\\
&=\mathrm{E} \left( \sum_{i=1}^n{\left( X_i^2-2\bar{X}X_i+\bar{X}^2 \right)} \right)\\
&=\mathrm{E} \left( \sum_{i=1}^n{\left( X_i^2-2\mu X_i+ \mu^2 + 2\mu X_i - \mu^2 -2\bar{X}X_i+\bar{X}^2 \right)} \right)\\
&=\mathrm{E} \left( \sum_{i=1}^n{\left( {(X_i-\mu)}^2 + 2\mu X_i - \mu^2 -2\bar{X}X_i+\bar{X}^2 \right)} \right)\\
&=\sum_{i=1}^n \mathrm{E}{(X_i-\mu)}^2
+2\mu \sum_{i=1}^n\mathrm{E}(X_i)
-n\mu^2
+\mathrm{E} \left( \sum_{i=1}^n{\left( -2\bar{X}X_i+\bar{X}^2 \right)} \right)\\
&= n\sigma^2 +2n\mu^2 -n\mu^2
+\mathrm{E} \left( \sum_{i=1}^n{\left( -2\bar{X}X_i+\bar{X}^2 \right)} \right)\\
\mathrm{E}\left(\sum_{i=1}^n X_i\bar{X}\right)
&=\frac{1}{n}\mathrm{E}\left( \sum_{i=1}^n X_i\sum_{j=1}^n X_j \right)
=\frac{1}{n}\mathrm{E}\left( \sum_{i=1}^n X_i^2 + 2\sum_{i<j} X_i X_j \right)\\
&=\frac{1}{n}\sum_{i=1}^n \mathrm{E}(X_i^2)+\frac{1}{n}2\binom{n}{2}{\left(\mathrm{E}X\right)}^2\\
&=\mathrm{E}(X_i^2)+(n-1){\left(\mathrm{E}X\right)}^2\\
\mathrm{E}\left(\sum_{i=1}^n \bar{X}^2\right)
&=n\mathrm{E}\left(\bar{X}^2\right)
=n\mathrm{E}\left( \frac{1}{n^2} {\left( \sum_{i=1}^n X_i \right)}^2 \right)
=\frac{1}{n}\mathrm{E}{\left( \sum_{i=1}^n X_i \right)}^2\\
&=\frac{1}{n}\mathrm{E}{\left( \sum_{i=1}^n X_i^2+2\sum_{i<j}X_i X_j \right)}\\
&=\mathrm{E}(X_i^2)+(n-1){\left(\mathrm{E}X\right)}^2\\
\mathrm{E}\left( (n-1)S_n^2 \right)
&= n\sigma^2 +n\mu^2
+\mathrm{E} \left( \sum_{i=1}^n{\left( -2\bar{X}X_i+\bar{X}^2 \right)} \right)\\
&= n\sigma^2 +n\mu^2
-\left( \mathrm{E}(X_i^2)+(n-1){\left(\mathrm{E}X\right)}^2 \right)\\
&= n\sigma^2 +n\mu^2 -\mathrm{E}(X_i^2)-(n-1)\mu^2\\
&= n\sigma^2 +\mu^2 -\mathrm{E}(X_i^2)
= n\sigma^2 +{(\mathrm{E}X_i)}^2 -\mathrm{E}(X_i^2)
= n\sigma^2 -\sigma^2 = (n-1)\sigma^2\\
\mathrm{E}( S_n^2 ) &= \sigma^2
\end{aligned}
$$

## ch7 Joint Distribution

## ch8 Transformations

### PDF transformation in variable substitution (with good smooth,bijective,differentiable properties)

For two continuous random variables $X,Y$ such that $X=g(Y)$.  
Let $F_X(x)=P(X<x),F_Y(y)=P(Y<y)$ be the CDFs of $X,Y$ respectively
and $f_X(x)=\lim_{\delta\to 0}\frac{P(|X-x|<\delta)}{2\delta}=\lim_{\delta\to 0}\frac{F_X(x+\delta)-F_X(x-\delta)}{2\delta}=F_X'(x),f_Y(y)=F_Y'(y)$ be the PDFs.  

Consider the probability that $X$ falls in $I_X=(x,x+\delta_X)$ and the corresponding interval $I_Y=(y,y+\delta_Y)$.  
where $\delta_Y=g'(x)\delta_X$  or $\mathrm{d}y=g'(x)\mathrm{d}x$

$$
\begin{aligned}
P(X\in I_X) &= P(Y\in I_Y) \\
f_X(x)|\mathrm{d}x| &= f_Y(y)|\mathrm{d}y| \\
f_Y(y) &= f_X(x) \left|\frac{\mathrm{d}x}{\mathrm{d}y}\right|
\end{aligned}
$$

For multi-variate case, $\vec y = \vec{g}(\vec x)$, similarly, consider the probability that $X$ falls in some region $D_x$ it is identical to the probability that $Y$ falls in the transformed region $D_y$.  
We have $\mathrm{d}x_1\mathrm{d}x_2\ldots \mathrm{d}x_n=\left|\frac{\partial (y_1,y_2\ldots y_n)}{\partial (x_1,x_2\ldots x_n)}\right|\mathrm{d}y_1\mathrm{d}y_2\ldots \mathrm{d}y_n$

$$
\begin{aligned}
P(X\in D_X) &= P(Y\in D_Y)\\
f_X(\vec x)\sigma_X&= f_Y(\vec y)\sigma_Y\\
f_X(\vec x)\mathrm{d}x_1\mathrm{d}x_2\ldots\mathrm{d}x_n &= f_Y(\vec y)\mathrm{d}y_1\mathrm{d}y_2\ldots \mathrm{d}y_n\\
f_Y(\vec y) &= f_X(\vec x)\left|\frac{\partial (x_1,x_2\ldots x_n)}{\partial (y_1,y_2\ldots y_n)}\right|
\end{aligned}
$$

**note:** make use of "chain-rules" (the Jacobian of composed function is the product of Jacobian) and "inverse transformation" (Jacobian of inverse is the inverse of the Jacobian).

### uncorrelated can not imply independent

Let $X\sim \mathcal{N}(0,1)$ be a standard normal random variable,  
$Y=X^2$.  

$$
\mathrm{Cov}(X,Y)=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)
=\mathbb{E}(X^3)-\mathbb{E}(X)\mathbb{E}(X^2)
=0^3-0\mathbb{E}(X^2)
=0
$$

**Special case:** For normal r.v.s, uncorrelated is equivalent to independent.

### Covariance, Correaltion examples

> Let $X,Y$\sim\mathrm{Expo}(1)$ be two i.i.d. r.v.s.  
> Find the correlation between $M=\max(X,Y)$ and $L=\min(X,Y)$.

Consider two independent arrivals with rate $\lambda=1$,  
the first arrival time has a $\mathrm{Expo}(1+1)$ distribution
and the next arrival, by memoryless property, has a $\mathrm{Expo}(1)$ distribution and its is independent of the first arrival.  

(The Poisson clock model/story)

We have $L\sim \mathrm{Expo}(2),M-L\sim\mathrm{Expo}(1)$ and $L,M-L$ is independent.

$$
\begin{aligned}
\mathrm{Cov}(L,M)
&
=\mathrm{Cov}(L,M-L+L)
=\mathrm{Cov}(L,M-L)+\mathrm{Cov}(L,L)\\
&
=0+\mathrm{Var}(L)=\frac{1}{2^2}=\frac{1}{4}\\
\mathrm{Cov}(M,M)
&
=\mathrm{Cov}(M-L+L,M-L+L)\\
&
=\mathrm{Cov}(M-L,M-L)+\mathrm{Cov}(L,L)+2\mathrm{Cov}(M-L,L)\\
&
=\frac{1}{1^2}+\frac{1}{2^2}+0
=\frac{5}{4}
\end{aligned}
$$

Thus

$$
\mathrm{Corr}(L,M)
=\frac{\mathrm{Cov}(L,M)}
{\sqrt{\mathrm{Cov}(L,L)\mathrm{Cov}(M,M)}}
=\frac{1/4}{\sqrt{\frac{1}{4}\cdot \frac{5}{4}}}
=\frac{1}{\sqrt 5}=\frac{\sqrt 5}{5}
$$

### Hyper-Geometric distribution variance

> $X\sim\mathrm{HGeom}(w,b,n)$.  
> we have $w$ white balls and $b$ black balls in a bag,  
> draw $n$ balls from that bag, count the number of white balls tobe $X$.

Let $A_i$ be the event that the $i$-th drawn ball is white, and $I_i$ be the corresponding indicator  
We have $I_i\sim \mathrm{Bern}\left(\frac{w}{w+b}\right)=\mathrm{Bern}(p)$, let $q=1-p=\frac{b}{w+b}$

$$
\begin{aligned}
\mathrm{Var}(X)
&=\mathrm{Var}\left( \sum_{i=1}^n I_i \right)
=\sum_{i=1}^n\sum_{j=1}^n \mathrm{Cov}(I_i,I_j)\\
&=\sum_{i=1}^n \mathrm{Var}(I_i)+2\sum_{i<j}\mathrm{Cov}(I_i,I_j)\\
\text{(Symmetry)}
&=n\mathrm{Var}(I_1)+2\binom{n}{2}\mathrm{Cov}(I_1,I_2)\\
\mathrm{Var}(I_i)
&=pq=\frac{w}{w+b}\frac{b}{w+b}\\
\mathrm{Cov}(I_1,I_2)
&=\mathbb{E}(I_1 I_2)-\mathbb{E}(I_1)\mathbb{E}(I_2)\\
&=\mathbb{P}(A_1\cap A_2)-\mathbb{P}(A_1)\mathbb{P}(A_2)\\
&=\frac{w}{w+b}\frac{w-1}{w+b-1}-p^2\\
\end{aligned}
$$

### multi-variate normal: MVN

A random vector $\vec X=(X_1,X_2\ldots X_n)$ is said to be a MVN
iff $\mathrm{span}(X_1,X_2\ldots X_n)$, all the linear combination of the components, all have normal distribution (a constant value $c$ is viewed as $\mathcal{N}(c,0)$)

#### examples

- $X\sim \mathcal{N}(0,1), Y=SX, P(S=1)=P(S=-1)=\frac{1}{2}$.  
  The marginal distribution of $X$ and $Y$ are both $\mathcal{N}(0,1)$,  
  but $(X,Y)$ is not a MVN since $P(X+Y=0)=P(S=-1)=\frac{1}{2}$.
- $X,Y\sim \mathcal{N}(0,1), Z=X+Y$ then $(X,Y,Z)$ is a MVN, although $Z$ is not independent from $(X,Y)$

#### BVN generating

Generate a BVN $(U,V)$ from two independent $\mathcal{N}(0,1)$ r.v.s $X,Y$, where $U,V\sim \mathcal{N}(0,1)$ and $\mathrm{Cov}(U,V)=\rho$  

Suppose that $U=aX+bY,V=cX+dY$

$$
\begin{cases}
\mathbb{E}(U)=\mathrm{E}(V)=a+b=c+d=1\\
\mathrm{Var}(U)=\mathrm{Var}(V)=a^2+b^2=c^2+d^2=1\\
\mathrm{Cov}(U,V)=\mathrm{Cov}(aX+bY,cX+dY)
=ac\mathrm{Var}(X)+bd\mathrm{Var}(Y)+0+0
=ac+bd
=\rho
\end{cases}
$$

Then

$$\begin{cases}
a=1\\
b=0\\
c=\rho\\
d=\sqrt{1-\rho^2}
\end{cases}
$$
is a solution.

#### BVN MGF

Let $(X,Y)$ be a BVN, where the marginal distribution is
$X\sim\mathcal{N}(\mu_x,\sigma_x^2)$,$Y\sim\mathcal{N}(\mu_y,\sigma_y^2)$
and $\mathrm{Corr}(X,Y)=\rho$.  
Then $Z=t_x X + t_y Y$ is a normal random variable $Z\sim\mathcal{N}(\mu,\sigma^2)$.

$$
\begin{aligned}
\mu
&=\mathbb{E}(Z)=t_x \mathbb{E}(x)+t_y \mathbb{E}(y) = t_x\mu_x+t_y\mu_y\\
\sigma^2
&=\mathrm{Var}(Z)=\mathrm{Cov}(t_x X+ t_y Y,t_x X+ t_y Y)
=t_x^2 \sigma_x^2 +t_y^2 \sigma_y^2+2t_xt_y\mathrm{Cov}(X,Y)\\
M_{X,Y}(t_x,t_y)
&=\mathbb{E}(e^{t_x X+ t_y Y})=\mathbb{E}(e^Z)
=\exp\left(
\mu + \frac{1}{2}\sigma^2
\right)\\
&=\exp\left(
\mu_x t_x + \mu_y t_y
+\frac{1}{2}
\left(
\sigma_x^2 t_x^2 + \sigma_y^2 t_y^2
+2\sigma_x t_x \sigma_y t_y \rho
\right)
\right)\\
\end{aligned}
$$

The marginal MGF of $X,Y$ are

$$
\begin{aligned}
M_X(t_x)
&=\exp\left( \mu_x t_x  + \frac{1}{2}\sigma_x^2 t_x^2 \right)\\
M_Y(t_y)
&=\exp\left( \mu_y t_y  + \frac{1}{2}\sigma_y^2 t_y^2 \right)\\
M_X(t_x)M_Y(t_y)
&=\exp\left( \mu_x t_x+\mu_y t_y  + \frac{1}{2}(\sigma_x^2 t_x^2 + \sigma_y^2 t_y^2) \right)\\
\end{aligned}
$$

The joint/marginal MGF determines the joint/marginal distribution.  

$\mathrm{Corr}(X,Y)=\rho=\frac{\mathrm{Cov}(X,Y)}{\sigma_x \sigma_y}=0$  
**iff**  
the product of marginal MGFs is equal to the joint MGF.  
**iff**  
the product of marginal PDFs is equal to the joint PDF.  

This is a significant result: In MVN, uncorrelated is equivalent to independent.  
(Generally speaking, uncorrelated is weaker than independent).

### normal samples: sample mean and sample variance are independent

$X_1,X_2\ldots X_n\sim \mathcal{N}(\mu,\sigma^2)$ are $n$ i.i.d. r.v.s.  
Therefore $\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i$ is a $\mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right)$ random variable.  

Consider $\vec{X}=(\bar X,X_1-\bar X,X_2-\bar X\ldots X_n-\bar X)$ is a linear combination of $(X_1,X_2\ldots X_n)$, which is a MVN.
Thus $\vec{X}$ is a MVN.  
Consider the covariance between $\bar X$ and $X_i-\bar X$.  

$$
\begin{aligned}
\mathrm{Cov}(\bar X,X_k-\bar X)
&=\mathrm{Cov}(\bar X,X_k)-\mathrm{Cov}(\bar X,\bar X)\\
&=\frac{1}{n}\mathrm{Cov}\left(\sum_{i=1}^n X_i, X_k\right) - \frac{\sigma^2}{n}\\
&=-\frac{\sigma^2}{n}+\frac{1}{n}\sum_{i=1}^n \mathrm{Cov}(X_i,X_k)\\
&=-\frac{\sigma^2}{n}+\frac{1}{n}\sum_{i=1}^n [i=k]\sigma^2\\
&=-\frac{\sigma^2}{n}+\frac{\sigma^2}{n}=0
\end{aligned}
$$

In MVN, uncorrelated components are independent.  
So $\bar X$ is independent of $X_i-\bar{X}$ for all $1\leq i\leq n$.  
(sample mean, distance from mean are independent)

Consider $S_n=\frac{1}{n-1}\sum_{i=1}^n {(X_i-\bar X)}^2$, is a function of $(X_1-\bar X,X_2-\bar X,\ldots X_n-\bar X)$  
so it is independent of $\bar{X}$.  
Sample mean and sample variance are independent.

### generating Normals, the Box-Muller method

Let $U\sim \mathrm{Unif}(0,2\pi), T\sim \mathrm{Expo}(1)$ be independent r.v.s  

$$
\begin{cases}
X=\sqrt{2 T}\cos U\\
Y=\sqrt{2 T}\sin U\\
\end{cases}
$$

Then $X,Y$ are i.i.d. $\mathcal{N}(0,1)$ r.v.s

$$
\begin{aligned}
\begin{vmatrix} \frac{\partial (x,y)}{\partial (u,t)} \end{vmatrix}
&=\begin{vmatrix}
-\sqrt{2t}\sin u  &  \frac{\cos u}{\sqrt{2t}} \\
 \sqrt{2t}\cos u  &  \frac{\sin u}{\sqrt{2t}} \\
\end{vmatrix}=-1\\
f_{X,Y}(x,y)
&=f_{U,T}(u,t) \begin{Vmatrix} \frac{\partial (u,t)}{\partial (x,y)} \end{Vmatrix}
=f_{U}(u)f_{T}(t) \begin{Vmatrix} \frac{\partial (u,t)}{\partial (x,y)} \end{Vmatrix}\\
&=\frac{1}{2\pi} \exp{\left(-\frac{x^2+y^2}{2}\right)}\frac{1}{|-1|}\\
&=\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2}\cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}y^2}\\
\implies f_{X,Y}(x,y)&=f_X(x)f_Y(y)
\quad
\begin{cases}
f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2}\\
f_Y(y) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}y^2}
\end{cases}
\end{aligned}
$$

## ch9 Conditional Expectation

## ch10 Inequalities and Limit Theorems

## ch11 Markov Chain

## ch12 MCMC

## ch13 Poisson Process

## MISC

### Poisson approximation

If $X_n\sim \mathrm{Bin}(n,\frac{\lambda}{n})$ then as $n\to \infty$, we have $X_{\infty}\sim \mathrm{Pois}(\lambda)$.  

For $n$ (weakly) independent rare events $A_1,A_2\ldots A_n$ where $\mathbb{E}(I(A_i))=P(A_i)=p_i$ is small,
and $\lambda=\sum_{i} p_i$.
Let $X=\sum_{i}I(A_i)$ approximately, has a Poisson distribution $\mathrm{Pois}(\lambda)$.

### Poisson process and Exponential distribution

- number of arrivals in an interval of length $t$ is a $\mathrm{Pois}(\lambda t)=e^{-\lambda t}\frac{{(\lambda t)}^n}{n!}$.
- time between two arrivals has an Exponential distribution.
- disjoint intervals are independent process.
- sum of poisson process is a poisson process.
- $X,Y$ are independent Poisson r.v.s, then $X+Y$ has a Poisson distribution and $X|X+Y=n$ has a Binomial distribution.

### expected value of geometric distribution

$\Pr(X=i) = {(1-p)}^{i-1}p$ for $p=1,2,3\ldots$

#### approach 1

let $q=1-p$

$$
\begin{aligned}
E(X)
&=\sum_{i=1}^\infty i q^{i-1} (1-q)\\
&=\lim_{n\to\infty}\sum_{i=1}^n i q^{i-1} (1-q)\\
&=\lim_{n\to\infty}\sum_{i=1}^n i q^{i-1} - \sum_{i=1}^n i q^i\\
&=\lim_{n\to\infty}\sum_{i=0}^{n-1} (i+1) q^i - \sum_{i=1}^n i q^i\\
&=\lim_{n\to\infty} (0+1) q^0-n q^n +\sum_{i=1}^{n-1}q^i\\
&=1+\frac{q}{1-q} = \frac{1}{1-q} = \frac{1}{p}
\end{aligned}
$$

#### approach 2

$$
\begin{aligned}
E(X)
&=\sum_{i=1}^\infty i\Pr(X=i)\\
&=\lim_{n\to \infty}\sum_{i=1}^n i\Pr(X=i)\\
&=\lim_{n\to \infty}\sum_{i=1}^n \Pr(X=i)\sum_{j=1}^i 1\\
&=\lim_{n\to \infty}\sum_{j=1}^n \sum_{i=j}^n \Pr(X=i)\\
&=\lim_{n\to \infty}\sum_{j=1}^n \Pr(j\leq X\leq n)\\
(?)&=\sum_{j=1}^\infty \Pr(X\geq j)\\
\end{aligned}
$$

$$
\begin{aligned}
\Pr(X\geq i)
&={(1-p)}^{i-1}\\
E(X)
&=\sum_{i=1}^\infty {(1-p)}^{i-1}=\frac{1}{1-(1-p)}=\frac{1}{p}
\end{aligned}
$$

#### extension

Similarly, for positive continuous random variable $X$ and its PDF $f(x)$:

$$
E(X)
=\int_0^{+\infty} xf(x)\mathrm{d}x
=\int_0^{+\infty} \int_0^x 1 f(x)\mathrm{d}y\mathrm{d}x
=\int_0^{+\infty} \Pr(X\geq x)\mathrm{d}x
$$

### conditional independence in bayesian network

see

- [zhihu王乐: 概率图模型之贝叶斯网络](https://zhuanlan.zhihu.com/p/30139208)
- wikipedia
- conditional probability chain rule:  
$$
\begin{aligned}
P(A_1A_2A_3\ldots A_n)
&=P(A_1)P(A_2\ldots A_n |A_1)\\[1em]
&=P(A_1)\ P(A_3\ldots A_n |A_1,A_2)P(A_2 |A_1)\\
&=P(A_1)P(A_2)P(A_3\ldots A_n |A_1,A_2)P(A_2 |A_1)\\[1em]
&=P(A_1)P(A_2|A_1)\ P(A_4\ldots A_n|A_1,A_2,A_3)P(A_3|A_1A_2)\\
&=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)P(A_4\ldots A_n|A_1,A_2,A_3)\\[1em]
&=\ldots\\
&=\prod_{i=1}^n P(A_i|A_1\ldots A_{i-1})
\end{aligned}
$$

#### the structure and data in the network

- DAG, random variable on the vertices, conditional probability on the edges.
- suppose that $A_1,A_2\ldots A_n$ are the ancestors of $B$ in $G=(E,V)$  
  i.e. $\{A_1,A_2\ldots A_n\}=\{v\in V\mid (v,B)\in E\}$  
  then we have a conditional distribution $P(C| A_1\ldots A_n)$  
  $P(C=c_i| A_1=a_1,A_2=a_2\ldots A_n=a_n)$

#### head to head

```plaintext
A -> X
B -> X
C -> X
```

the network(graph) gives $P(X=x| A=a,B=b,C=c)$

#### tail to tail

```plaintext
X -> A
X -> B
X -> C
```

the network(graph) gives $P(A=a| X=x), P(B=b| X=x), P(C=c| X=x)$

#### head to tail

```plaintext
A -> B -> C
```

the network(graph) gives $P(B=b| A=a), P(C=c| B=b)$

### PGF example: first occurance of a given pattern in coin filp sequence

> Given a sequence $(a_1,a_2\ldots a_n\ldots )$
> where $a_i$ are i.i.d. Bernoulli random variable with prameter $p$, let $q=1-p$.  
>
> Let $X$ be the first time that $111$ is witnessed i.e. $X=k$ iff $(a_{k-2},a_{k-1},a_{k})=(1,1,1)$ and $\forall i<k, (a_{i-2},a_{i-1},a_i)\neq (1,1,1)$.  
>
> Find the moments of $X$.

Let $r_n=P(X=n)$ for all natural number $n$. We have the initial condition $p_0=p_1=p_2=0,p_3=p^3$.  

For every $n>3$.  
Condition on the first step (condition on $a_1$), LOTP:

$$
\begin{aligned}
r_n = P(X=n)
&=P(X=n|a_1=0)P(a_1=0)+P(X=n|a_1=1)P(a_1=1)\\
&=q P(X=n|a_1=0) + pP(X=n|a_1=1)
\end{aligned}
$$

- For $P(X=n|a_1=0)$, since the Bernoulli process is memory less, this is equivalent to $P(X=n-1)=r_{n-1}$
- For $P(X=n|a_1=1)$, we have to consider $a_2$.  
  1. If $a_2=0$, this is another fresh start $P(X=n|a_1=1,a_2=0)=P(X=n-2)$
  2. If $a_2=1$, then $a_3=0$, otherwise we would have $X=3$, $P(X=n|a_1=1,a_2=1)=P(X=n-3)P(a_3=0)$

Thus (LOTP with extra condition):

$$
\begin{aligned}
&q P(X=n|a_1=0) + p P(X=n|a_1=1)\\
=&q r_{n-1} + p
  \left[
    P(X=n|a_1=1,a_2=0)P(a_2=0|a_1=1)
   +P(X=n|a_1=1,a_2=1)P(a_2=1|a_1=1)
  \right]\\
=&q r_{n-1} + p
  \left[
    P(X=n|a_1=1,a_2=0)q
   +P(X=n|a_1=1,a_2=1)p
  \right]\\
=&q r_{n-1} + p
  \left[
    q P(X=n-2)
   +p P(X=n|a_1=a_2=1,a_3=0) P(a_3=0|a_1=a_2=1)
  \right]\\
=&q r_{n-1} + p
  \left[
    q P(X=n-2)
   +pq P(X=n-3)
  \right]\\
=& q r_{n-1}+pq r_{n-2} + p^2 q r_{n-3}
\end{aligned}
$$

Let $g(z)=\mathbb{E}(z^X)=\sum_{n=0}^\infty r_n z^n$ be the PGF of $X$,

$$
\begin{aligned}
g(z)&=\sum_{n=0}^\infty r_n z^n\\
&=r_0 z^0 + r_1 z^1 + r_2 z^2 + r_3 z^3 + \sum_{n=4}^\infty r_n z^n\\
&=p^3 z^3 + \sum_{n=4}^\infty (q r_{n-1}+pq r_{n-2} + p^2 q r_{n-3}) z^n\\
&=p^3 z^3 + q z \sum_{n=4}^\infty r_{n-1}z^{n-1} + pq z^2 \sum_{n=4}^\infty  r_{n-2} z^{n-2} + p^2q z^3 \sum_{n=4}^\infty r_{n-3}z^{n-3}\\
&=p^3 z^3 + q z \sum_{n=3}^\infty r_{n}z^{n} + pq z^2 \sum_{n=2}^\infty  r_{n} z^{n} + p^2q z^3 \sum_{n=1}^\infty r_{n}z^{n}\\
&=p^3 z^3 + q z g(z) + pq z^2 g(z) + p^2q z^3 g(z)\\
\left( 1-qz-pqz^2-p^2qz^3 \right)g(z)
&=p^3 z^3\\
g(z)
&=\frac{p^3 z^3}{1-qz-pqz^2-p^2qz^3}
\end{aligned}
$$

And $g^{(k)}(1)=\mathbb{E}\left(\prod_{i=0}^{k-1}(X-i)\right)$ use Stirling numbers to convert $x^{\underline k}$ to $x^{k}$,
or use an iterative approach
$\mathbb{E}(X)=g'(1),
\mathbb{E}(X^2)={\left[ \left(z g' \right)' \right]}_{z=1},
\mathbb{E}(X^3)={\left[ \left(z \left( zg' \right)' \right)' \right]}_{z=1},
$.

**Alternative solution:** Markov chain (KMP automata + transition probability graph)

### order statistics of i.i.d continuous random variables

Let $X_1,X_2\ldots X_n$ be $n$ i.i.d. continuous r.v.s whose CDF is $F(x)$.  
In a random experiment, the result is $S=(x_1,x_2\ldots x_n)$, let $X_{(k)}$ be the $k$-th smallest value in $S$.

#### CDF

Then the CDF of $X_{(k)}$ is

$$
F_{X_{(k)}}(x)=P(X_{(k)}\leq x)=\sum_{i=k}^n \binom{n}{i} {(F(x))}^i {(1-F(x))}^{n-i}
$$

For any $x$, consider the random variable $Y=\sum_{i=1}^n \left[ X_i\leq x \right]$,
then $Y\sim\mathrm{Bin}\left(n,F(x)\right)$.  
If $X_{k}\leq x$ then there are at least $k$ variable in $X_1\ldots X_n$ such that $X_i\leq x$,  
therefore $P(X_{k}\leq x)=P(Y\geq k)$.

#### (marginal) PDF

Then, let's find the PDF of $X_{(k)}$, let $f_k(x)$ be the PDF,  
then for a small interval $\delta$, we have$P(x-\delta<X_{(k)}<x+\delta)=2\delta f_{k}(x)$.  

$$
\begin{aligned}
2\delta f_k(x)
&=P(x-\delta<X_{(k)}<x+\delta)\\
\text{(LOTP. Which $X_i$ becomes the $k$-th)}
&=\sum_{i=1}^n P(x-\delta < X_{(k)} < x+\delta | X_{(k)}=X_i)P(X_{(k)}=X_i)\\
\text{(i.i.d. continuous random variable symmetry)}
&=n P(x-\delta < X_{(k)} < x+\delta | X_{(k)}=X_1)P(X_{(k)}=X_1)\\
\text{(i.i.d. continuous random variable symmetry)}
&=P(x-\delta < X_{(k)} < x+\delta | X_{(k)}=X_1)\\
\text{(by definition)}
&=\frac{ P(x-\delta < X_{(k)} < x+\delta ,\ X_{(k)}=X_1) }{P(X_{(k)}=X_1)} \\
\text{($\ast$)}
&=\frac{ 2\delta f_{X_1}(x)\ \binom{n-1}{k-1}{(F(x))}^{k-1}{(1-F(x))}^{n-k} }{1/n} \\
&=n\ 2\delta f(x)\ \binom{n-1}{k-1}{(F(x))}^{k-1}{(1-F(x))}^{n-k}\\
f_{X_{(k)}}(x)
&=nf(x) \binom{n-1}{k-1}{(F(x))}^{k-1}{(1-F(x))}^{n-k}\\
\end{aligned}
$$

Consider the $(\ast)$ step. $P(x-\delta < X_{(k)} < x+\delta ,\ X_{(k)}=X_1)$.
To make that happen, both of the follwing two properties have to be satisfied.

- Place $X_1$ in the range $(x-\delta,x+\delta)$.  
- For $X_2,X_3\ldots X_n$. Select $(k-1)$ $X_j$s that fall on to the left of $x$, while the other $(n-k)$ $X_j$s fall on to the right of $x$.

Since all $X_j$s are independent, the probability is

$$
2\delta f(x) \ \binom{n-1}{k-1}{(F(x))}^{k-1}{(1-F(x))}^{n-k}
$$

#### (joint) PDF

$$
f_{(X_{(1)},X_{(2)}\ldots, X_{(n)})}(x_1,x_2\ldots x_n)
=n! \prod_{i=1}^n f(x_i)
$$

1. For all permutation of $[n]$ $p_1,p_2\ldots p_n$,
   let $X_{(i)}=X_{p_i}$.  
2. Put $X_{p_i}$ at $(x_i-\delta,x_i+\delta)$

Since $X_1,X_2\ldots X_n$ are independent, the probability of the product event is the product of the probability of each event.

#### identity

$$
\begin{aligned}
P(X_{(k)}<x)
&=F_{X_{(k)}}(x)
=\sum_{i=k}^n \binom{n}{i} {(F(x))}^i {(1-F(x))}^{n-i}\\
&=\int_{\infty}^x f_{X_{(k)}}(t)\mathrm{d}{t}
=\int_{\infty}^x n\binom{n-1}{k-1}f(t){F(t)}^{k-1}{(1-F(t))}^{n-k}\mathrm{d}{t}
\end{aligned}
$$

#### ordered statistics of $\mathrm{Unif}(0,1)$

For $n$ i.i.d. $\mathrm{Unif}(0,1)$ r.v.s. $X_1,X_2\ldots X_n$

$$
\begin{aligned}
P(X_{(k)}<x)
&=\sum_{i=k}^n\binom{n}{i}{(F(x))}^i{(1-F(x))}^{n-i}\\
&=\sum_{i=k}^n\binom{n}{i}x^i {(1-x)}^{n-i}\\
&=\int_{0}^{x}n\binom{n-1}{k-1}f(x) {(F(x))}^{i-1} {(1-F(x))}^{n-i}\mathrm{d}t\\
&=\int_{0}^{x}n\binom{n-1}{k-1} x^{i-1} {(1-x)}^{n-i}\mathrm{d}t\\
\end{aligned}
$$

we have a few more related identities, $\sum_{i=k}^n,\sum_{i=0}^k,\int_0^x,\int_x^1$.

#### two indentities

$$
\begin{aligned}
&\forall 0<p<1,\, k\in \mathbb{N}
\qquad
&\sum_{i=0}^k \binom{n}{i}p^i {(1-p)}^{n-i}
=(n-k)\binom{n}{k}\int_p^1 x^k{(1-x)}^{n-k-1}\mathrm{d}x\\
&\forall 0\leq k\leq n,\ k,n\in \mathbb{N}
\qquad
&\int_0^1 \binom{n}{k}x^k{(1-x)}^{n-k}\mathrm{d}x
=\frac{1}{n+1}
\end{aligned}
$$

1. For the first one.  
   Consider the ordered statistics of $n$ i.i.d. $\mathrm{Unif}(0,1)$ r.v.s.  
   This is the probability that $X_{(k+1)}>p$
2. For the second one  
   Consider $n+1$ i.i.d. $\mathrm{Unif}(0,1)$ r.v.s $X_1,X_2\ldots X_n,X_{n+1}$.  
   $P(X_1=X_{(k+1)}| X_1=x)$ is $\binom{n}{k}x^k{(1-x)}^{n-k}$.  
   By LOTP, LHS is $P(X_1=X_{(k+1)})=\frac{1}{n+1}$ i.i.d. continuous r.v.s symmetry property.

### note: abs and min/max

$$
\begin{aligned}
\max(x,y)+\min(x,y)
&=x+y\\
\max(x,y)-\min(x,y)
&=\begin{cases}
x-y & x\geq y\\
y-x & x<y\\
\end{cases}
=|x-y|\\
x+y + |x-y|
&=(\max(x,y)+\min(x,y))+(\max(x,y)-\min(x,y))=\max(x,y)|\\
x+y - |x-y|
&=(\max(x,y)+\min(x,y))-(\max(x,y)-\min(x,y))=\min(x,y)|\\
\end{aligned}
$$

### Beta and Gamma

#### Beta integral and Gamma integral

$$
\begin{aligned}
\beta(a,b)&=\int_0^1 x^{a-1} {(1-x)}^{b-1}\mathrm{d}x & (a,b>0)\\
\Gamma(a) &=\int_0^{+\infty} x^{a-1}e^{-x}\mathrm{d}x=\int_0^{+\infty} x^{a}e^{-x}\frac{\mathrm{d}x}{x} & (a>0)\\
\end{aligned}
$$

Useful properties

- $\beta(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$
- $\Gamma(a+1)=a\Gamma(a)$
- $\Gamma(1)=1,\Gamma(n)=(n-1)!$

#### Beta distribution and Gamma distribution

- $X\sim \mathrm{Beta}(a,b)$ if the PDF of $X$ is $f(x)=\frac{1}{\beta(a,b)} x^{a-1}{(1-x)}^{b-1}$ for $0<x<1$
- $X\sim \mathrm{Gamma}(a,1)$ if the PDF of $X$ is $f(x)=\frac{1}{\Gamma(a)} x^{a-1}e^{-x}=\frac{1}{\Gamma(a)}x^a e^{-x} \frac{1}{x}$ for $x>0$

if $X\sim \mathrm{Beta}(a,b)$ then, $Y=1-x\sim\mathrm{Beta}(b,a)$

$$
\begin{aligned}
\mathbb{E}(X)
&=\frac{1}{\beta(a,b)}\int_0^1 x\cdot x^{a-1}{(1-x)}^{b-1}\mathrm{d}x
=\frac{1}{\beta(a,b)}\int_0^1 x^{a}{(1-x)}^{b-1}\mathrm{d}x\\
&=\frac{\beta(a+1,b)}{\beta(a,b)}
=\frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)}\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}
=\frac{\Gamma(a+1)}{\Gamma(a)}\frac{\Gamma(a+b)}{\Gamma(a+b+1)}\\
&=\frac{a}{a+b}\\
f_Y(y)&=f_x(x)\begin{vmatrix} \frac{\mathrm{d}x}{\mathrm{d}y} \end{vmatrix}=f_x(1-y)|-1|\\
&=\frac{1}{\beta(a,b)} {(1-y)}^{a-1} y^b
=\frac{1}{\beta(b,a)} y^b {(1-y)}^{a-1}
\end{aligned}
$$

If $X\sim\mathrm{Gamma}(a,1)$ then $Y=\frac{X}{\lambda}\sim\mathrm{Gamma}(a,\lambda)$

$$
\begin{aligned}
f_Y(y)
&=f_X(x)\frac{\mathrm{d} x}{\mathrm{d} y}
=\frac{1}{\Gamma(a)} {(\lambda y)}^{a-1}e^{-\lambda y}\cdot \lambda
=\frac{1}{\Gamma(a)} {(\lambda y)}^{a} e^{-\lambda y}\frac{1}{y}\\
\mathbb{E}(X)
&=\int_0^{+\infty}\frac{x}{\Gamma(a)}x^{a-1}e^{-x}\mathrm{d}x
=\int_0^{+\infty}\frac{1}{\Gamma(a)}x^{a}e^{-x}\mathrm{d}x
=\frac{\Gamma(a+1)}{\Gamma(a)}
=a\\
\mathbb{E}(X^2)
&=\int_0^{+\infty}\frac{1}{\Gamma(a)}x^{a+1}e^{-x}\mathrm{d}x
=\frac{\Gamma(a+2)}{\Gamma(a)}
=a(a+1)\\
\mathbb{E}(X^k)
&=\int_0^{+\infty}\frac{1}{\Gamma(a)}x^{a+1}e^{-x}\mathrm{d}x
=\frac{\Gamma(a+k)}{\Gamma(a)}
=a^{\overline k}\\
\mathrm{Var}(X)&=\mathbb{E}(X)=a
\end{aligned}
$$

#### Gamma, sum of independent Exponentials, arrival times in Poisson process

- For $n$ i.i.d. $\mathrm{Expo}(\lambda)$ r.v.s. $X_1,X_2\ldots X_n$,
the sum $Y=\sum_{i=1}^n X_i$ have a $\mathrm{Gamma}(n,\lambda)$ distribution.
- In a Poisson process with prameter $\lambda$, the $k$-th arrival time $T_k$ has $\mathrm{Gamma}(k,\lambda)$ distribution.  
  (note: $T_1<T_2\ldots T_n$, are dependent.)

#### Beta-Gamma connection

If $X\sim \mathrm{Gamma}(a,\lambda),Y\sim \mathrm{Gamma}(b,\lambda)$ are independent.  
Let $T=X+Y, W=\frac{X}{X+Y}$, then $T\sim \mathrm{Gamma}(a+b,\lambda),W\sim\mathrm{Beta}(a,b)$ are independent.

$$
\begin{aligned}
(x,y)&=(tw,t(1-w)) \quad (t,w) = (x+y,\frac{x}{x+y})\\
\begin{vmatrix}
\frac{\partial (x,y)}{\partial (t,w)}
\end{vmatrix}
&=\begin{vmatrix}
w   & t\\
1-w &  -t
\end{vmatrix}
=-wt-t(1-w)=-t
\end{aligned}
$$

$$
\begin{aligned}
f_{T,W}(t,w)
&=f_{X,Y}(x,y)
\begin{Vmatrix}
\frac{\partial (x,y)}{\partial (t,w)}
\end{Vmatrix}
=f_{X}(x)f_{Y}(y)
\begin{Vmatrix}
\frac{\partial (x,y)}{\partial (t,w)}
\end{Vmatrix}\\
&=
\frac{1}{\Gamma(a)} {(\lambda x)}^a e^{-\lambda x} \frac{1}{x}
\cdot
\frac{1}{\Gamma(b)} {(\lambda y)}^b e^{-\lambda y} \frac{1}{y}
\cdot t\\
&=
\frac{1}{\Gamma(a)} {(\lambda tw)}^a e^{-\lambda tw} \frac{1}{tw}
\cdot
\frac{1}{\Gamma(b)} {(\lambda t(1-w))}^b e^{-\lambda t(1-w)} \frac{1}{t(1-w)}
\cdot t\\
&=
\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} w^{a-1} {(1-w)}^{b-1}
\cdot
\frac{1}{\Gamma(a+b)} {(\lambda t)}^{a+b}e^{-\lambda t} \frac{1}{t}\\
\implies
f_{T,W}(t,w)&=f_T(t)f_W(w)
\quad
\begin{cases}
f_T(t)&=\frac{1}{\Gamma(a+b)} {(\lambda t)}^{a+b}e^{-\lambda t} \frac{1}{t}\\
f_W(w)&=\frac{1}{\beta(a,b)}w^{a-1}{(1-w)}^{b-1}\\
\end{cases}
\end{aligned}
$$

#### Story of Beta and Gamma distribution

- **Beta: ordered statistics of uniforms**  
  $\beta(a+1,b+1)=\int_0^1 x^a{(1-x)}^b\mathrm{d}x=\frac{\Gamma(a+1)\Gamma(b+1)}{\Gamma(a+b+2)}=\frac{a! b!}{(a+b+1)!}$  
  Suppose that $X_1\ldots X_a, Z, Y_1\ldots Y_b$ are i.i.d. $\mathrm{Unif}(0,1)$ r.v.s.  
  The probability that $\max(X_1\ldots X_a) < Z < \min(Y_1\ldots Y_b)$ is $x^a {(1-x)}^b$
- **Beta: (unknown) success probability of Bernoulli trials**  
  In $\mathrm{Beta}(a,b)$, the parameters $a,b$ are called pesudo counts, where $a$ is the number of successful trials and $b$ is the number of failed trails.  
  If $p\sim \mathrm{Beta}(a,b), X|p\sim \mathrm{Bin}(n,p)$ and $X=k$ is observed.  
  The distribution of success probability get updated to $\mathrm{Beta}(a+k,b+(n-k))$
- **Gamma: waiting time until the n-th arrival**  
  $\mathrm{Gamma}(1,\lambda)=\mathrm{Expo}(\lambda)$  
  If $X_1\ldots X_n\sim\mathrm{Expo}(\lambda)$ are i.i.d. r.v.s.
  Then $\sum_i X_i\sim\mathrm{Gamma}(n,\lambda)$.
- **Beta: the fraction of waiting time**  
  $X\sim \mathrm{Gamma}(a,\lambda), Y\sim \mathrm{Gamma}(b,\lambda)$, waiting for $a+b$ independent arrivals with rate $\lambda$  
  Then $\frac{X}{X+Y}\sim\mathrm{Beta}(a,b)$
- **Gamma: (unknown) rate of Poisson process**  
  $\lambda \sim \mathrm{Gamma}(r,b)$ is the prior distribution of the arrival rate $\lambda$  
  Then $X|\lambda \sim\mathrm{Pois}(\lambda t)$, the number of arrivals in $[0,t]$ for a Poisson process with parameter $\lambda$.  
  If $X=n$ is observed, the posterior distribution of $\lambda$ is updated to $\mathrm{Gamma}(r+n,b+t)$  
  $r$ is the number of arrivals, $b$ is the total waiting time. The average rate is $\mathbb{E}(\lambda)=\left(\frac{r}{b}\to \frac{r+n}{b+t}\right)$

### Conjugacy

reference: [wikipedia: conjugacy prior](https://en.wikipedia.org/wiki/Conjugate_prior)

#### Beta-Binomial

If $p$ have a $\mathrm{Beta}(a,b)$ prior distribution, and $X|p=\mathrm{Bin}(n,p)$.  
If $X=k$ is observed, the posterior distribution of $p$, $p|X=k$ is $\mathrm{Beta}(a+k,b+n-k)$  
The positive real number $a,b$ are called _pesudo count_.  

The prior distribution can be history records, uniform, or arbitarily picked things.

$$
\begin{aligned}
f_{p|X=k}(t,k)
&=\frac{P(X=k|p=t)f_p(t)}{P(X=k)}\\
&=\frac{
\binom{n}{k} t^k {(1-t)}^{n-k}
\,
\frac{1}{\beta(a,b)} t^{a-1}{(1-t)}^{b-1}
}{P(X=k)}\\
&=\frac{\binom{n}{k}}{\beta(a,b) P(X=k)} t^{a+k-1} {(1-t)}^{b+n-k-1}
\end{aligned}
$$

The conditional PDF is also a valid PDF which integrates to $1$, thus $\frac{\binom{n}{k}}{\beta(a,b) P(X=k)}=\beta(a+k,b+n-k)$.
The posterior distribution is $\mathrm{Beta}(a+k,b+n-k)$

note: if a $\mathrm{Unif}(0,1)$ prior, we can use $\mathrm{Beta}(1,1)$.

#### Dirichlet-Multinomial

An extension of the Beta-Binomial conjugacy.

- prior distribution: $\mathrm{Dirichlet}(a_1,a_2\ldots a_k)$  
  $f(p_1,p_2\ldots p_k)=\frac{\Gamma\left(\sum_i a_i\right)}{\prod_i \Gamma(a_i)}\prod_i p_i^{a_i}$
- observation: $(X_1,X_2\ldots X_k)|(p_1,p_2\ldots p_k) \sim \mathrm{Multi}(n,(p_1,p_2\ldots p_k))$  
  $P(X_1=n_1\ldots X_k=n_k)=\frac{n!}{\prod_i n_i!} p_i^{x_i}$ where $\sum_i n_i=n$
- prior distribution: $\mathrm{Dirichlet}(a_1+X_1,a_2+X_2\ldots a_k+X_k)$  
  $f(\vec p|\vec X)$

#### Gamma-Poisson

- The prior distribution of $\lambda$ is $\mathrm{Gamma}(r,b)$  
- Obersvation $X|\lambda\sim \mathrm{Pois}(\lambda t)$. The number of arrivals in a Poisson process with parameter $\lambda$ in $[0,t]$ time.
- The posterior distribution $\lambda|X \sim \mathrm{Gamma}(r+n,b+t)$

$$
\begin{aligned}
f_{\lambda|X}(\lambda|X=n)
&=\frac{P(X=n|\lambda)f(\lambda)}{P(X=n)}\\
&=\frac{
e^{-\lambda t}\frac{{(\lambda t)}^n}{n!}
\cdot
\frac{1}{\Gamma(r)} {(b\lambda)}^{r} e^{-b\lambda}\frac{1}{\lambda}
}{P(X=n)}\\
&=\frac{\Gamma(r+n) b^r}{\Gamma(r)n!P(X=n) {(b+t)}^{r+n}}
\cdot
\frac{1}{\Gamma(r+n)} {((b+t)\lambda)}^{r+n} e^{-(b+t)\lambda} \frac{1}{\lambda}\\
&\propto
\frac{1}{\Gamma(r+n)} {((b+t)\lambda)}^{r+n} e^{-(b+t)\lambda} \frac{1}{\lambda}
\end{aligned}
$$

Other factors are independent of $\lambda$, so they can be viewed as normalizing constants.  
Thus, the PDF is

$$
f_{\lambda|X}(\lambda|X=n)
=\frac{1}{\Gamma(r+n)} {((b+t)\lambda)}^{r+n} e^{-(b+t)\lambda} \frac{1}{\lambda}
$$

Which is $\mathrm{Gamma}(n+r,b+t)$

$$
\frac{r}{b}
\to
\frac{r+n}{b+t}
$$
