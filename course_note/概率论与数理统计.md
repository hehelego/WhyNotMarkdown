 概率论与数理统计 note

> textbook: Introduction to Probability
> Joseph K. Blitzstein, Jessica Hwang

[TOC]

## ch0 introduction

## ch1 probability and counting

### birthday problem

$$
\begin{aligned}
p_{k,n}
&=\prod_{i=0}^{k-1}\frac{n-i}{n}
=\prod_{i=0}^{k-1}\left(1 - \frac{i}{n} \right)\\
e^x &\approx x+1\quad (|x|\to 0)\\
p_{n,k}
&\approx \prod_{i=0}^{k-1} e^{-\frac{i}{n}}
=\prod_{i=0}^{k-1} \exp\left( {-\frac{i}{n}} \right)
=\exp\left( {-\frac{k(k-1)}{n}} \right)
\end{aligned}
$$

Find the relation between n and k when $p_{n,k}\leq \frac{1}{2}$, using the approximation.  

$$
\begin{aligned}
&p_{n,k}
\approx \exp\left( -\frac{k(k-1)}{n} \right)
\approx \exp\left( -\frac{k^2}{n} \right)
\leq \frac{1}{2}\\
&-\frac{k^2}{n} \leq -\ln 2\quad k\geq \sqrt{n\ln 2}
\end{aligned}
$$

### probability axioms

$(P,S)$ probability space: probability function and probability sample space.

- $\forall A\subseteq S\quad 0\leq P(A)\leq 1$
- $P(\varnothing)=0,\ P(S)=1$
- $A_1,A_2\ldots A_n\ldots$ are disjoint subsets of $S$  
  $P\left(\bigcup_{i=1}^\infty A_i\right)=\sum_{i=1}^\infty P(A_i)$

#### deriving properties of probability from the axioms

complement rule:  
$A\cap A^c = \varnothing, A\cup A^c = S$, so $P(A\cup A^c)=P(A)+P(A^c)=P(S)=1$,  
thus $P(A)=1-P(A^c)$

inclusive-exclusive principle:  
$A\setminus (A\cap B), B$ is a partition of $A\cup B$. $P(A\cup B)=P(A\setminus (A\cap B))+P(B)$  
$A\setminus (A\cap B), A\cap B$ is a partition of $A$. $P(A)=P(A\setminus (A\cap B))+P(A\cap B)$  
$P(A\cup B)=P(A)+P(B)-P(A\cap B)$

subset monotonous:  
$A\subseteq B$.  
$B\setminus A,A$ is a partition of $B$, so $P(B)=P(A)+P(B\setminus A)\geq P(A)+0=P(A)$

Boole inequality: $P(\bigcup_{i=1}^\infty A_i)\leq \sum_{i=1}^\infty A_i$  
Let $B_1=A_1,B_2=A_2\cap \overline{A_1},B_3=A_3\cap \overline{A_1\cap A_2}\ldots B_n=A_n\cap \overline{\bigcap_{i=1}^{n-1} A_i}\ldots$  
The sets $B_i$ are disjoint and $\forall n \bigcup_{i=1}^n A_i=\bigcup_{i=1}^n B_i$  
$$
P\left(\bigcup_{i=1}^\infty A_i\right)
= P\left(\bigcup_{i=1}^\infty B_i\right)
= \sum_{i=1}^n P(B_i)
\leq \sum_{i=1}^n P(A_i)
$$

## ch2 conditional probability

LOTP, LOTE (law of total probability/exepectation): let $(A_1,A_2\ldots A_n)$ be a partition of $S$  
partition: $\bigcup A_i=S$, $i\neq j\rightarrow A_i\cap A_j=\varnothing$

$$
\begin{aligned}
P(B) &= \sum_{i=1}^n P(B | A_i) P(A_i)\\
E(B) &= \sum_{i=1}^n E(B | A_i) P(A_i)
\end{aligned}
$$

Bayes's rule:

$$
P(A|B)
=\frac{P(B|A)P(A)}{P(B)}
=\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B| A^c)P(A^c)}
$$

### conditional probability as a probability function

In a probability space $(S,P)$ and an event $C\subseteq S$.  
Define the function $\hat P(A)=P(A\mid C)$, 
then $(S,\hat P)$ consists a probability space.

- Bayes' rule: $\hat P(A|B)=\frac{\hat P(B|A)P(A)}{P(B)}$  
  which is $P(A\mid B,C)=\frac{P(B | A,C)P(A | C)}{P(B|C)}$
- complement: $\hat P(A^c)=1-\hat P(A)$
- inclusive-exclusive: $\hat P(A\cup B)=\hat P(A)+\hat P(B)-\hat P(A\cap B)$

### independence

Definition: two events $A,B$ are said to be independent if "prior probability equals to posterior probability"

$$
P(A\cap B)=P(A)P(B)
\quad
P(A\mid B)=\frac{P(AB)}{P(B)}=P(A)
$$

the following statements are equivalent

- $A$ and $B$ are independent
- $A^c$ and $B$ are independent
- $A$ and $B^c$ are independent
- $A^c$ and $B^c$ are independent

#### comparison: pairwise independence and independence

PID doesn't imply ID, on the contrary ID implies PID.

e.g. Consider tossing two independent fair coin.

- A: the first coin lands on head.
- B: the second coin lands on head.
- C: the two coin give the same result.

A, B, C are pairwise independent but not independent.  
($A\cap B$ implies $C$)

$$
P(A,B,C)=\frac{1}{4}\neq P(A)P(B)P(C)=\frac{1}{8}
$$

### conditional independence

A, B are said to be conditional independent given $E$ if.
$$
P(A,B | E) = P(A|E)P(B|E)
$$

#### comparison: conditional independence and independence

##### CID can't imply ID

Given a fair coin and a biased coin (land on head at 1/2, 3/4 probability respectively)  
We choose a coin randomly and flip it twice.

- F: the fair coin is chosen
- A: the first tosses landing heads
- B: the second tosses landing heads

$A, B$ are conditional independent given $F$.  

$$
P(A,B | F)=\frac{1}{2}\frac{1}{2} = P(A|F)P(B|F)
$$

But $A,B$ are not independent.

$$
\begin{aligned}
P(A) &= P(A | F)P(F) + P(A | F^c)P(F^c) = \frac{1}{2}\frac{1}{2} + \frac{3}{4}\frac{1}{2} = \frac{5}{8}\\
P(B) &= P(A | F)P(F) + P(A | F^c)P(F^c) = \frac{1}{2}\frac{1}{2} + \frac{3}{4}\frac{1}{2} = \frac{5}{8}\\
P(AB)
&= P(AB | F)P(F) + P(AB | F^c)P(F^c)
=\frac{1}{2} {\left( \frac{1}{2} \right)}^2
+\frac{1}{2} {\left( \frac{3}{4} \right)}^2
=\frac{13}{32}\\
P(A)P(B) &= \frac{25}{64}
\end{aligned}
$$

##### ID can't imply CID

Given two independent events $A, B$ where $0<P(A)=p,P(B)=q<1$, let $C=A\bar{B}+\bar{A}B$.  
$A, \bar{B}$ are not are conditional independent given $C$.

$$
\begin{aligned}
P(A|C) &= \frac{P(A\cap C)}{P(C)}=\frac{P(A\bar{B})}{P(A\bar{B}+\bar{A}B)}=\frac{p(1-q)}{p(1-q)+q(1-p)}>0\\
P(B|C) &= \frac{P(B\cap C)}{P(C)}=\frac{q(1-p)}{p(1-q)+q(1-p)}>0\\
P(A\cap B|C) &= 0\\
P(A|C)P(B|C) &> 0
\end{aligned}
$$

##### CID given E v.s. CID given complement of E

e.g. The courses in university are either graded base on effort or graded base on random number generator.

- G: a course grades students according to academical performance.
- W: work hard.
- A: receive A plus grade.

$A, W$ are conditional independent given $G^c$, but are related(not independent) given $G$.

## ch3 random variables
