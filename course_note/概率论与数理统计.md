# 概率论与数理统计 note

> textbook: Introduction to Probability
> Joseph K. Blitzstein, Jessica Hwang

[TOC]

## ch0 introduction

## ch1 probability and counting

### birthday problem

$$
\begin{aligned}
p_{k,n}
&=\prod_{i=0}^{k-1}\frac{n-i}{n}
=\prod_{i=0}^{k-1}\left(1 - \frac{i}{n} \right)\\
e^x &\approx x+1\quad (|x|\to 0)\\
p_{n,k}
&\approx \prod_{i=0}^{k-1} e^{-\frac{i}{n}}
=\prod_{i=0}^{k-1} \exp\left( {-\frac{i}{n}} \right)
=\exp\left( {-\frac{k(k-1)}{n}} \right)
\end{aligned}
$$

Find the relation between n and k when $p_{n,k}\leq \frac{1}{2}$, using the approximation.  

$$
\begin{aligned}
&p_{n,k}
\approx \exp\left( -\frac{k(k-1)}{n} \right)
\approx \exp\left( -\frac{k^2}{n} \right)
\leq \frac{1}{2}\\
&-\frac{k^2}{n} \leq -\ln 2\quad k\geq \sqrt{n\ln 2}
\end{aligned}
$$

### probability axioms

$(P,S)$ probability space: probability function and probability sample space.

- $\forall A\subseteq S\quad 0\leq P(A)\leq 1$
- $P(\varnothing)=0,\ P(S)=1$
- $A_1,A_2\ldots A_n\ldots$ are disjoint subsets of $S$  
  $P\left(\bigcup_{i=1}^\infty A_i\right)=\sum_{i=1}^\infty P(A_i)$

#### deriving properties of probability from the axioms

complement rule:  
$A\cap A^c = \varnothing, A\cup A^c = S$, so $P(A\cup A^c)=P(A)+P(A^c)=P(S)=1$,  
thus $P(A)=1-P(A^c)$

inclusive-exclusive principle:  
$A\setminus (A\cap B), B$ is a partition of $A\cup B$. $P(A\cup B)=P(A\setminus (A\cap B))+P(B)$  
$A\setminus (A\cap B), A\cap B$ is a partition of $A$. $P(A)=P(A\setminus (A\cap B))+P(A\cap B)$  
$P(A\cup B)=P(A)+P(B)-P(A\cap B)$

subset monotonous:  
$A\subseteq B$.  
$B\setminus A,A$ is a partition of $B$, so $P(B)=P(A)+P(B\setminus A)\geq P(A)+0=P(A)$

Boole inequality: $P(\bigcup_{i=1}^\infty A_i)\leq \sum_{i=1}^\infty A_i$  
Let $B_1=A_1,B_2=A_2\cap \overline{A_1},B_3=A_3\cap \overline{A_1\cap A_2}\ldots B_n=A_n\cap \overline{\bigcap_{i=1}^{n-1} A_i}\ldots$  
The sets $B_i$ are disjoint and $\forall n \bigcup_{i=1}^n A_i=\bigcup_{i=1}^n B_i$  
$$
P\left(\bigcup_{i=1}^\infty A_i\right)
= P\left(\bigcup_{i=1}^\infty B_i\right)
= \sum_{i=1}^n P(B_i)
\leq \sum_{i=1}^n P(A_i)
$$

## ch2 conditional probability

LOTP, LOTE (law of total probability/expectation): let $(A_1,A_2\ldots A_n)$ be a partition of $S$  
partition: $\bigcup A_i=S$, $i\neq j\rightarrow A_i\cap A_j=\varnothing$

$$
\begin{aligned}
P(B) &= \sum_{i=1}^n P(B | A_i) P(A_i)\\
E(B) &= \sum_{i=1}^n E(B | A_i) P(A_i)
\end{aligned}
$$

Bayes's rule:

$$
P(A|B)
=\frac{P(B|A)P(A)}{P(B)}
=\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B| A^c)P(A^c)}
$$

### conditional probability as a probability function

In a probability space $(S,P)$ and an event $C\subseteq S$.  
Define the function $\hat P(A)=P(A\mid C)$, 
then $(S,\hat P)$ consists a probability space.

- Bayes' rule: $\hat P(A|B)=\frac{\hat P(B|A)P(A)}{P(B)}$  
  which is $P(A\mid B,C)=\frac{P(B | A,C)P(A | C)}{P(B|C)}$
- complement: $\hat P(A^c)=1-\hat P(A)$
- inclusive-exclusive: $\hat P(A\cup B)=\hat P(A)+\hat P(B)-\hat P(A\cap B)$

### independence

Definition: two events $A,B$ are said to be independent if "prior probability equals to posterior probability"

$$
P(A\cap B)=P(A)P(B)
\quad
P(A\mid B)=\frac{P(AB)}{P(B)}=P(A)
$$

the following statements are equivalent

- $A$ and $B$ are independent
- $A^c$ and $B$ are independent
- $A$ and $B^c$ are independent
- $A^c$ and $B^c$ are independent

#### comparison: pairwise independence and independence

PID doesn't imply ID, on the contrary ID implies PID.

e.g. Consider tossing two independent fair coin.

- A: the first coin lands on head.
- B: the second coin lands on head.
- C: the two coin give the same result.

A, B, C are pairwise independent but not independent.  
($A\cap B$ implies $C$)

$$
P(A,B,C)=\frac{1}{4}\neq P(A)P(B)P(C)=\frac{1}{8}
$$

### conditional independence

A, B are said to be conditional independent given $E$ if.
$$
P(A,B | E) = P(A|E)P(B|E)
$$

#### comparison: conditional independence and independence

##### CID can't imply ID

Given a fair coin and a biased coin (land on head at 1/2, 3/4 probability respectively)  
We choose a coin randomly and flip it twice.

- F: the fair coin is chosen
- A: the first tosses landing heads
- B: the second tosses landing heads

$A, B$ are conditional independent given $F$.  

$$
P(A,B | F)=\frac{1}{2}\frac{1}{2} = P(A|F)P(B|F)
$$

But $A,B$ are not independent.

$$
\begin{aligned}
P(A) &= P(A | F)P(F) + P(A | F^c)P(F^c) = \frac{1}{2}\frac{1}{2} + \frac{3}{4}\frac{1}{2} = \frac{5}{8}\\
P(B) &= P(A | F)P(F) + P(A | F^c)P(F^c) = \frac{1}{2}\frac{1}{2} + \frac{3}{4}\frac{1}{2} = \frac{5}{8}\\
P(AB)
&= P(AB | F)P(F) + P(AB | F^c)P(F^c)
=\frac{1}{2} {\left( \frac{1}{2} \right)}^2
+\frac{1}{2} {\left( \frac{3}{4} \right)}^2
=\frac{13}{32}\\
P(A)P(B) &= \frac{25}{64}
\end{aligned}
$$

##### ID can't imply CID

Given two independent events $A, B$ where $0<P(A)=p,P(B)=q<1$, let $C=A\bar{B}+\bar{A}B$.  
$A, \bar{B}$ are not are conditional independent given $C$.

$$
\begin{aligned}
P(A|C) &= \frac{P(A\cap C)}{P(C)}=\frac{P(A\bar{B})}{P(A\bar{B}+\bar{A}B)}=\frac{p(1-q)}{p(1-q)+q(1-p)}>0\\
P(B|C) &= \frac{P(B\cap C)}{P(C)}=\frac{q(1-p)}{p(1-q)+q(1-p)}>0\\
P(A\cap B|C) &= 0\\
P(A|C)P(B|C) &> 0
\end{aligned}
$$

##### CID given E v.s. CID given complement of E

e.g. The courses in university are either graded base on effort or graded base on random number generator.

- G: a course grades students according to academical performance.
- W: work hard.
- A: receive A plus grade.

$A, W$ are conditional independent given $G^c$, but are related(not independent) given $G$.

### example: distribution of the sum of independent identical dice roll

> source:
>
> - Introduction to probability
> - Joseph K. Blitzstein, Jessica Hwang
> - second edition
> - chapter2, exercise 48
>
> [see also](https://www.zhihu.com/question/336062847) `从0点出发,每次向正方向随机走1~k步,求踩到x的概率. 为什么x趋于无穷大时,概率为2/(k+1)`

#### problem statement

A fair die is rolled repeatedly, and a running total is kept (which is, at each time, the total of all the rolls up until that time). Let $p_n$ be the probability that the running total is ever exactly $n$ (assume the die will always be rolled enough time so that the running total will eventually exceed $n$, but it may or may not ever equal $n$).

- write down a recursive relation for $p_n$.
- find $\lim_{n\to \infty}p_n$

#### solution

the RR

$$
p_n=
\begin{cases}
0& n<0\\
1& n=1\\
\frac{1}{6}\left(\sum_{i=1}^6 p_{n-i}\right) & n>1\\
\end{cases}
$$

Every time we roll a die, the sum increase by $\frac{\sum_{i=1}^6 i}{6}=\frac{7}{2}$ on average.
It can be viewed/interpreted in this way "in every 7 consecutive numbers the sum lands on 2 of the 7 numbers", so the probability converges to $\frac{2}{7}$.

A somewhat more convincing proof

$$
\begin{aligned}
 &p_{n+1}+2p_{n+2}+3p_{n+3}+4p_{n+4}+5p_{n+5}+6p_{n+6}\\
=&p_{n+1}+2p_{n+2}+3p_{n+3}+4p_{n+4}+5p_{n+5}+(p_n+p_{n+1}+2p_{n+2}+3p_{n+3}+4p_{n+4}+5p_{n+5})\\
=&p_n+2p_{n+1}+3p_{n+2}+4p_{n+3}+5p_{n+4}+6p_{n+5}\\
=&\cdots\\
=&p_{-5}+2p_{-4}+3p_{-3}+4p_{-2}+5p_{-1}+6p_{0}\\
=&6
\end{aligned}
$$

taking limit when $n\to \infty$ on both side, $6=\sum_{i=1}^6 i\lim_{n\to\infty}p_n$, so $p\to \frac{2}{7}$

## ch3 random variables

## ch4 expectation

### more on indicators

For $n$ (arbitary, independence is not required)events $A_1,A_2\ldots A_n$ and their indicators $I_1,I_2\ldots I_n$.  
Let $X=\sum_{i=1}^n I_i$, consider $\binom{X}{k}$

- $k=0$, $\binom{X}{0}=1$ and $E\left(\binom{X}{0}\right)=1$
- $k=1$, $\binom{X}{1}=X$ and $E\left(\binom{X}{1}\right)=X$
- $k=2$, $\binom{X}{2}=\frac{X(X-1)}{2}=\sum_{i<j}I_iI_j$ and $E\left(\binom{X}{2}\right)=\sum_{i<j}P(A_i\cap A_j)$  
  $
  \begin{aligned}
  X(X-1)
  &=X^2-X\\
  &=\left(2\sum_{i<j}I_iI_j+\sum_i I_i^2\right)-\sum_i I_i\\
  &=\left(2\sum_{i<j}I_iI_j+\sum_i I_i\right)-\sum_i I_i\\
  &=2\sum_{i<j}I_iI_j
  \end{aligned}
  $
- $k=3$, $\binom{X}{3}=\frac{X(X-1)(X-2)}{6}=\sum_{i<j<k}I_iI_jI_k$ and $E\left(\binom{X}{3}\right)=\sum_{i<j<k}P(A_i\cap A_j\cap A_k)$  
  $
  \begin{aligned}
  X(X-1)(X-2)
  &=\left(2\sum_{i<j}I_iI_j\right)\left(\sum_i I_k - 2\right)\\
  &=2\left(\sum_{i<j\land k=i}I_iI_jI_k+\sum_{i<j\land k\neq i\land k\neq j}I_iI_jI_k+\sum_{i<j\land k=j}I_iI_jI_k-2\sum_{i<j}I_iI_j\right)\\
  &=2\left(\sum_{i<j\land k\neq i\land k\neq j}I_iI_jI_k\right)\\
  &=2\left(\sum_{p<q<r}I_pI_qI_r\sum_{i<j\land k\neq i\land k\neq j}[(p,q,r)\in \{(i,j,k),(i,k,j),(k,i,j)\}]\right)\\
  &=2\left(3\sum_{i<j<k}I_iI_jI_k\right)=6\sum_{i<j<k}I_iI_jI_k
  \end{aligned}
  $
- $k=m$, $\binom{X}{m}=\sum_{i_1<i_2\ldots i_m}\prod_{j=1}^m I_j$ and $E\left(\binom{X}{m}\right)=\sum_{i_1<i_2\ldots i_m}P(\prod_{j=1}^m A_{i_j})$

#### evaluate moments of binomial distribution

Given that $X\sim \mathrm{Bin}(n,p)$.  
Let $A_j$ be the event that the $j$th Bernoulli trial success and $I_j$ be the corresponding indicator.  

$
\begin{aligned}
\binom{X}{k}
&=\sum_{p_1<p_2\ldots p_k}\sum_{i=1}^k I_{p_i}\\
\mathbb{E}\binom{X}{k}
&=\sum_{p_1<p_2\ldots p_k}P\left(\prod_{i=1}^k A_{p_i}\right)
=\sum_{p_1<p_2\ldots p_k}p^k=\binom{n}{k}p^k\\
X^m
&=\sum_{i=1}^m \binom{X}{i}i!\begin{Bmatrix}m\\ i\end{Bmatrix}\\
\mathbb{E}(X^m)
&=\sum_{i=1}^m \binom{n}{i}p^i i!\begin{Bmatrix}m\\ i\end{Bmatrix}
=\sum_{i=1}^m n^{\underline i}p^i\begin{Bmatrix}m\\ i\end{Bmatrix}\\
\end{aligned}
$

## ch5 continuous random variable

## ch6 moments

### sample variance is an unbiased estimation of the variance

Given $n$ independent identical distributed random variables (the samples) $X_1,X_2\ldots X_n$, where $\mathrm{E}(X_i)=\mu,\mathrm{Var}(X_i)=\sigma^2$
We have the nice relation that $\mathrm{E}\left(\frac{1}{n}\sum_{i=1}^n X_i^k\right)=\frac{1}{n}\sum_{i=1}^n \mathrm{E}(X_i^k)=\frac{1}{n}n\mathrm{E}(X_1^k)=\mathrm{E}(X^n)$ for all $k\in \mathbb{Z}$.  
That is, the expectation of sample moments are equal to the corresponding moments or sample moment is unbiased estimator of moment.

Thus
$$
\begin{aligned}
\mathrm{E}(\overline{X})
&=\mathrm{E}(\frac{1}{n}\sum_{i=1}^n X_i)
=\frac{1}{n}\sum_{i=1}^n \mathrm{E}(X_i)
=\mathrm{E}(X)=\mu\\
\mathrm{Var}(\overline{X})
&=\mathrm{Var}\left(\frac{1}{n}\sum_{i=1}^n X_i\right)
=\frac{1}{n^2}\sum_{i=1}^n \mathrm{Var}(X_i)
=\frac{n\sigma^2}{n^2}
=\frac{\sigma^2}{n}
\end{aligned}
$$

However, we can not find the expectation if the distribution is unknown,
(the only thing that we can get to known is the value of samples in a random experiment)
we have to use other estimator.

We define the **sample variance** and the **sample standard deviation** as follows:
$$
S_n^2=\frac{1}{n-1}\sum_{i=1}^n{\left(X_i-\bar{X}\right)}^2
\quad
S_n=\sqrt{S_n^2}
$$
It turns out that $\mathrm{E}(S_n^2)=\sigma^2$

$$
\begin{aligned}
S_n^2
&=\frac{1}{n-1}\sum_{i=1}^n{\left(X_i-\bar{X}\right)}^2\\
\mathrm{E}\left( (n-1)S_n^2 \right)
&=\mathrm{E} \left( \sum_{i=1}^n{\left(X_i-\bar{X}\right)}^2 \right)\\
&=\mathrm{E} \left( \sum_{i=1}^n{\left( X_i^2-2\bar{X}X_i+\bar{X}^2 \right)} \right)\\
&=\mathrm{E} \left( \sum_{i=1}^n{\left( X_i^2-2\mu X_i+ \mu^2 + 2\mu X_i - \mu^2 -2\bar{X}X_i+\bar{X}^2 \right)} \right)\\
&=\mathrm{E} \left( \sum_{i=1}^n{\left( {(X_i-\mu)}^2 + 2\mu X_i - \mu^2 -2\bar{X}X_i+\bar{X}^2 \right)} \right)\\
&=\sum_{i=1}^n \mathrm{E}{(X_i-\mu)}^2
+2\mu \sum_{i=1}^n\mathrm{E}(X_i)
-n\mu^2
+\mathrm{E} \left( \sum_{i=1}^n{\left( -2\bar{X}X_i+\bar{X}^2 \right)} \right)\\
&= n\sigma^2 +2n\mu^2 -n\mu^2
+\mathrm{E} \left( \sum_{i=1}^n{\left( -2\bar{X}X_i+\bar{X}^2 \right)} \right)\\
\mathrm{E}\left(\sum_{i=1}^n X_i\bar{X}\right)
&=\frac{1}{n}\mathrm{E}\left( \sum_{i=1}^n X_i\sum_{j=1}^n X_j \right)
=\frac{1}{n}\mathrm{E}\left( \sum_{i=1}^n X_i^2 + 2\sum_{i<j} X_i X_j \right)\\
&=\frac{1}{n}\sum_{i=1}^n \mathrm{E}(X_i^2)+\frac{1}{n}2\binom{n}{2}{\left(\mathrm{E}X\right)}^2\\
&=\mathrm{E}(X_i^2)+(n-1){\left(\mathrm{E}X\right)}^2\\
\mathrm{E}\left(\sum_{i=1}^n \bar{X}^2\right)
&=n\mathrm{E}\left(\bar{X}^2\right)
=n\mathrm{E}\left( \frac{1}{n^2} {\left( \sum_{i=1}^n X_i \right)}^2 \right)
=\frac{1}{n}\mathrm{E}{\left( \sum_{i=1}^n X_i \right)}^2\\
&=\frac{1}{n}\mathrm{E}{\left( \sum_{i=1}^n X_i^2+2\sum_{i<j}X_i X_j \right)}\\
&=\mathrm{E}(X_i^2)+(n-1){\left(\mathrm{E}X\right)}^2\\
\mathrm{E}\left( (n-1)S_n^2 \right)
&= n\sigma^2 +n\mu^2
+\mathrm{E} \left( \sum_{i=1}^n{\left( -2\bar{X}X_i+\bar{X}^2 \right)} \right)\\
&= n\sigma^2 +n\mu^2
-\left( \mathrm{E}(X_i^2)+(n-1){\left(\mathrm{E}X\right)}^2 \right)\\
&= n\sigma^2 +n\mu^2 -\mathrm{E}(X_i^2)-(n-1)\mu^2\\
&= n\sigma^2 +\mu^2 -\mathrm{E}(X_i^2)
= n\sigma^2 +{(\mathrm{E}X_i)}^2 -\mathrm{E}(X_i^2)
= n\sigma^2 -\sigma^2 = (n-1)\sigma^2\\
\mathrm{E}( S_n^2 ) &= \sigma^2
\end{aligned}
$$

## ch7 joint distribution

## ch8 transformations

## ch9 conditional expectation

## ch10 inequalities and limit theorems

## ch11 markov chain

## ch12 MCMC

## ch13 poisson process

## MISC

### expected value of geometric distribution

$\Pr(X=i) = {(1-p)}^{i-1}p$ for $p=1,2,3\ldots$

#### approach 1

let $q=1-p$

$$
\begin{aligned}
E(X)
&=\sum_{i=1}^\infty i q^{i-1} (1-q)\\
&=\lim_{n\to\infty}\sum_{i=1}^n i q^{i-1} (1-q)\\
&=\lim_{n\to\infty}\sum_{i=1}^n i q^{i-1} - \sum_{i=1}^n i q^i\\
&=\lim_{n\to\infty}\sum_{i=0}^{n-1} (i+1) q^i - \sum_{i=1}^n i q^i\\
&=\lim_{n\to\infty} (0+1) q^0-n q^n +\sum_{i=1}^{n-1}q^i\\
&=1+\frac{q}{1-q} = \frac{1}{1-q} = \frac{1}{p}
\end{aligned}
$$

#### approach 2

$$
\begin{aligned}
E(X)
&=\sum_{i=1}^\infty i\Pr(X=i)\\
&=\lim_{n\to \infty}\sum_{i=1}^n i\Pr(X=i)\\
&=\lim_{n\to \infty}\sum_{i=1}^n \Pr(X=i)\sum_{j=1}^i 1\\
&=\lim_{n\to \infty}\sum_{j=1}^n \sum_{i=j}^n \Pr(X=i)\\
&=\lim_{n\to \infty}\sum_{j=1}^n \Pr(j\leq X\leq n)\\
(?)&=\sum_{j=1}^\infty \Pr(X\geq j)\\
\end{aligned}
$$

$$
\begin{aligned}
\Pr(X\geq i)
&={(1-p)}^{i-1}\\
E(X)
&=\sum_{i=1}^\infty {(1-p)}^{i-1}=\frac{1}{1-(1-p)}=\frac{1}{p}
\end{aligned}
$$

#### extension

Similarly, for positive continuous random variable $X$ and its PDF $f(x)$:

$$
E(X)
=\int_0^{+\infty} xf(x)\mathrm{d}x
=\int_0^{+\infty} \int_0^x 1 f(x)\mathrm{d}y\mathrm{d}x
=\int_0^{+\infty} \Pr(X\geq x)\mathrm{d}x
$$

### conditional independence in bayesian network

see

- [zhihu王乐: 概率图模型之贝叶斯网络](https://zhuanlan.zhihu.com/p/30139208)
- wikipedia
- conditional probability chain rule:  
$$
\begin{aligned}
P(A_1A_2A_3\ldots A_n)
&=P(A_1)P(A_2\ldots A_n |A_1)\\[1em]
&=P(A_1)\ P(A_3\ldots A_n |A_1,A_2)P(A_2 |A_1)\\
&=P(A_1)P(A_2)P(A_3\ldots A_n |A_1,A_2)P(A_2 |A_1)\\[1em]
&=P(A_1)P(A_2|A_1)\ P(A_4\ldots A_n|A_1,A_2,A_3)P(A_3|A_1A_2)\\
&=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)P(A_4\ldots A_n|A_1,A_2,A_3)\\[1em]
&=\ldots\\
&=\prod_{i=1}^n P(A_i|A_1\ldots A_{i-1})
\end{aligned}
$$

#### the structure and data in the network

- DAG, random variable on the vertices, conditional probability on the edges.
- suppose that $A_1,A_2\ldots A_n$ are the ancestors of $B$ in $G=(E,V)$  
  i.e. $\{A_1,A_2\ldots A_n\}=\{v\in V\mid (v,B)\in E\}$  
  then we have a conditional distribution $P(C| A_1\ldots A_n)$  
  $P(C=c_i| A_1=a_1,A_2=a_2\ldots A_n=a_n)$

#### head to head

```plaintext
A -> X
B -> X
C -> X
```

the network(graph) gives $P(X=x| A=a,B=b,C=c)$

#### tail to tail

```plaintext
X -> A
X -> B
X -> C
```

the network(graph) gives $P(A=a| X=x), P(B=b| X=x), P(C=c| X=x)$

#### head to tail

```plaintext
A -> B -> C
```

the network(graph) gives $P(B=b| A=a), P(C=c| B=b)$

### PGF example: first occurance of a given pattern in coin filp sequence

> Given a sequence $(a_1,a_2\ldots a_n\ldots )$
> where $a_i$ are i.i.d. Bernoulli random variable with prameter $p$, let $q=1-p$.  
>
> Let $X$ be the first time that $111$ is witnessed i.e. $X=k$ iff $(a_{k-2},a_{k-1},a_{k})=(1,1,1)$ and $\forall i<k, (a_{i-2},a_{i-1},a_i)\neq (1,1,1)$.  
>
> Find the moments of $X$.

Let $r_n=P(X=n)$ for all natural number $n$. We have the initial condition $p_0=p_1=p_2=0,p_3=p^3$.  

For every $n>3$.  
Condition on the first step (condition on $a_1$), LOTP:

$$
\begin{aligned}
r_n = P(X=n)
&=P(X=n|a_1=0)P(a_1=0)+P(X=n|a_1=1)P(a_1=1)\\
&=q P(X=n|a_1=0) + pP(X=n|a_1=1)
\end{aligned}
$$

- For $P(X=n|a_1=0)$, since the Bernoulli process is memory less, this is equivalent to $P(X=n-1)=r_{n-1}$
- For $P(X=n|a_1=1)$, we have to consider $a_2$.  
  1. If $a_2=0$, this is another fresh start $P(X=n|a_1=1,a_2=0)=P(X=n-2)$
  2. If $a_2=1$, then $a_3=0$, otherwise we would have $X=3$, $P(X=n|a_1=1,a_2=1)=P(X=n-3)P(a_3=0)$

Thus (LOTP with extra condition):

$$
\begin{aligned}
&q P(X=n|a_1=0) + p P(X=n|a_1=1)\\
=&q r_{n-1} + p
  \left[
    P(X=n|a_1=1,a_2=0)P(a_2=0|a_1=1)
   +P(X=n|a_1=1,a_2=1)P(a_2=1|a_1=1)
  \right]\\
=&q r_{n-1} + p
  \left[
    P(X=n|a_1=1,a_2=0)q
   +P(X=n|a_1=1,a_2=1)p
  \right]\\
=&q r_{n-1} + p
  \left[
    q P(X=n-2)
   +p P(X=n|a_1=a_2=1,a_3=0) P(a_3=0|a_1=a_2=1)
  \right]\\
=&q r_{n-1} + p
  \left[
    q P(X=n-2)
   +pq P(X=n-3)
  \right]\\
=& q r_{n-1}+pq r_{n-2} + p^2 q r_{n-3}
\end{aligned}
$$

Let $g(z)=\mathbb{E}(z^X)=\sum_{n=0}^\infty r_n z^n$ be the PGF of $X$,

$$
\begin{aligned}
g(z)&=\sum_{n=0}^\infty r_n z^n\\
&=r_0 z^0 + r_1 z^1 + r_2 z^2 + r_3 z^3 + \sum_{n=4}^\infty r_n z^n\\
&=p^3 z^3 + \sum_{n=4}^\infty (q r_{n-1}+pq r_{n-2} + p^2 q r_{n-3}) z^n\\
&=p^3 z^3 + q z \sum_{n=4}^\infty r_{n-1}z^{n-1} + pq z^2 \sum_{n=4}^\infty  r_{n-2} z^{n-2} + p^2q z^3 \sum_{n=4}^\infty r_{n-3}z^{n-3}\\
&=p^3 z^3 + q z \sum_{n=3}^\infty r_{n}z^{n} + pq z^2 \sum_{n=2}^\infty  r_{n} z^{n} + p^2q z^3 \sum_{n=1}^\infty r_{n}z^{n}\\
&=p^3 z^3 + q z g(z) + pq z^2 g(z) + p^2q z^3 g(z)\\
\left( 1-qz-pqz^2-p^2qz^3 \right)g(z)
&=p^3 z^3\\
g(z)
&=\frac{p^3 z^3}{1-qz-pqz^2-p^2qz^3}
\end{aligned}
$$

And $g^{(k)}(1)=\mathbb{E}\left(\prod_{i=0}^{k-1}(X-i)\right)$ use Stirling numbers to convert $x^{\underline k}$ to $x^{k}$,
or use an iterative approach
$\mathbb{E}(X)=g'(1),
\mathbb{E}(X^2)={\left[ \left(z g' \right)' \right]}_{z=1},
\mathbb{E}(X^3)={\left[ \left(z \left( zg' \right)' \right)' \right]}_{z=1},
$.

**Alternative solution:** Markov chain (KMP automata + transition probability graph)
